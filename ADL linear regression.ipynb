{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import copy\n",
    "import time\n",
    "import pandas\n",
    "import numpy as np\n",
    "import scipy.io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maximum_absolute_scaling(df):\n",
    "    # copy the dataframe\n",
    "    df_scaled = df.copy()\n",
    "    # apply maximum absolute scaling\n",
    "    for column in df_scaled.columns:\n",
    "        df_scaled[column] = df_scaled[column]  / df_scaled[column].abs().max()\n",
    "    return df_scaled\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findRul (df,threshold):\n",
    "    sliding_window = 30 \n",
    "    unitCycle = df.groupby([ 0 ]).count()[ 1 ]\n",
    "    df=df.merge(unitCycle,on= 0 ,how= 'left' )\n",
    "    df[ 'rul' ]=df[ '1_y' ]-sliding_window-df[ '1_x' ]+ 1\n",
    "    df[ 'rul' ]=df[ 'rul' ].apply( lambda x: checkThreshold(x,threshold))\n",
    "    df.columns=np.arange(len(df.columns))\n",
    "    return df \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkThreshold (x,threshold):\n",
    "    if x > threshold:\n",
    "        return threshold\n",
    "    if x < 0 :\n",
    "        return 0\n",
    "    else :\n",
    "        return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #TRAINING DATA\n",
    "# ##load data\n",
    "# # data1 = scipy.io.loadmat('electricitypricing.mat')\n",
    "# # data = data1.get('data')\n",
    "# # n_feature = 8\n",
    "# data1 = pandas.read_csv(\"train_FD001.txt\", sep=' ', lineterminator='\\n', header = None)\n",
    "# data2 = data1.dropna(axis='columns')\n",
    "# data3 = findRul(data2,130)\n",
    "# data4 = data3.drop(columns=[26])\n",
    "# normalised_df = maximum_absolute_scaling(data4)\n",
    "# data = normalised_df.to_numpy()\n",
    "# print(type(data))\n",
    "# n_feature = 26\n",
    "# normalised_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>...</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.002058</td>\n",
       "      <td>0.476271</td>\n",
       "      <td>0.831354</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.947018</td>\n",
       "      <td>0.942122</td>\n",
       "      <td>0.921016</td>\n",
       "      <td>0.858933</td>\n",
       "      <td>0.639535</td>\n",
       "      <td>0.629338</td>\n",
       "      <td>...</td>\n",
       "      <td>0.999397</td>\n",
       "      <td>0.981966</td>\n",
       "      <td>0.837152</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.911839</td>\n",
       "      <td>0.973199</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.612188</td>\n",
       "      <td>0.616904</td>\n",
       "      <td>0.112821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.004115</td>\n",
       "      <td>0.595087</td>\n",
       "      <td>0.736342</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.891781</td>\n",
       "      <td>0.832717</td>\n",
       "      <td>0.781176</td>\n",
       "      <td>0.721587</td>\n",
       "      <td>0.482216</td>\n",
       "      <td>0.416474</td>\n",
       "      <td>...</td>\n",
       "      <td>0.848850</td>\n",
       "      <td>0.959333</td>\n",
       "      <td>0.986040</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.770781</td>\n",
       "      <td>0.801926</td>\n",
       "      <td>0.8493</td>\n",
       "      <td>0.361622</td>\n",
       "      <td>0.359835</td>\n",
       "      <td>0.112821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.006173</td>\n",
       "      <td>0.999810</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.857964</td>\n",
       "      <td>0.852921</td>\n",
       "      <td>0.833388</td>\n",
       "      <td>0.773635</td>\n",
       "      <td>0.267442</td>\n",
       "      <td>0.263304</td>\n",
       "      <td>...</td>\n",
       "      <td>0.999468</td>\n",
       "      <td>0.984668</td>\n",
       "      <td>0.852864</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.826196</td>\n",
       "      <td>0.926298</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.261647</td>\n",
       "      <td>0.270103</td>\n",
       "      <td>0.112821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.008230</td>\n",
       "      <td>0.999893</td>\n",
       "      <td>0.997862</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.857964</td>\n",
       "      <td>0.852859</td>\n",
       "      <td>0.833121</td>\n",
       "      <td>0.774600</td>\n",
       "      <td>0.267442</td>\n",
       "      <td>0.263304</td>\n",
       "      <td>...</td>\n",
       "      <td>0.999448</td>\n",
       "      <td>0.985615</td>\n",
       "      <td>0.849542</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.826196</td>\n",
       "      <td>0.926298</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.265928</td>\n",
       "      <td>0.261719</td>\n",
       "      <td>0.112821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.010288</td>\n",
       "      <td>0.833363</td>\n",
       "      <td>0.997625</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.866524</td>\n",
       "      <td>0.862037</td>\n",
       "      <td>0.841417</td>\n",
       "      <td>0.781974</td>\n",
       "      <td>0.374829</td>\n",
       "      <td>0.368811</td>\n",
       "      <td>...</td>\n",
       "      <td>0.999435</td>\n",
       "      <td>0.983046</td>\n",
       "      <td>0.844422</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.838791</td>\n",
       "      <td>0.930905</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.373961</td>\n",
       "      <td>0.374850</td>\n",
       "      <td>0.112821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>248</td>\n",
       "      <td>0.569959</td>\n",
       "      <td>0.999788</td>\n",
       "      <td>0.997743</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.857964</td>\n",
       "      <td>0.854583</td>\n",
       "      <td>0.848481</td>\n",
       "      <td>0.789928</td>\n",
       "      <td>0.267442</td>\n",
       "      <td>0.264692</td>\n",
       "      <td>...</td>\n",
       "      <td>0.999699</td>\n",
       "      <td>0.989729</td>\n",
       "      <td>0.857103</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.833753</td>\n",
       "      <td>0.926298</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.265173</td>\n",
       "      <td>0.262780</td>\n",
       "      <td>0.133333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>248</td>\n",
       "      <td>0.572016</td>\n",
       "      <td>0.476162</td>\n",
       "      <td>0.831948</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.947018</td>\n",
       "      <td>0.944187</td>\n",
       "      <td>0.929542</td>\n",
       "      <td>0.882067</td>\n",
       "      <td>0.639535</td>\n",
       "      <td>0.632115</td>\n",
       "      <td>...</td>\n",
       "      <td>0.999627</td>\n",
       "      <td>0.986584</td>\n",
       "      <td>0.842080</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.921914</td>\n",
       "      <td>0.973199</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.612692</td>\n",
       "      <td>0.614718</td>\n",
       "      <td>0.133333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>248</td>\n",
       "      <td>0.574074</td>\n",
       "      <td>0.833146</td>\n",
       "      <td>0.999169</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.866524</td>\n",
       "      <td>0.863310</td>\n",
       "      <td>0.852368</td>\n",
       "      <td>0.791187</td>\n",
       "      <td>0.374829</td>\n",
       "      <td>0.370199</td>\n",
       "      <td>...</td>\n",
       "      <td>0.999757</td>\n",
       "      <td>0.988293</td>\n",
       "      <td>0.853063</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.846348</td>\n",
       "      <td>0.930905</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.369932</td>\n",
       "      <td>0.370918</td>\n",
       "      <td>0.133333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>248</td>\n",
       "      <td>0.576132</td>\n",
       "      <td>0.476164</td>\n",
       "      <td>0.831354</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.947018</td>\n",
       "      <td>0.944482</td>\n",
       "      <td>0.926035</td>\n",
       "      <td>0.878828</td>\n",
       "      <td>0.639535</td>\n",
       "      <td>0.632115</td>\n",
       "      <td>...</td>\n",
       "      <td>0.999644</td>\n",
       "      <td>0.986390</td>\n",
       "      <td>0.841672</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.921914</td>\n",
       "      <td>0.973199</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.615462</td>\n",
       "      <td>0.616400</td>\n",
       "      <td>0.133333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>248</td>\n",
       "      <td>0.578189</td>\n",
       "      <td>0.833353</td>\n",
       "      <td>0.997862</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.866524</td>\n",
       "      <td>0.864056</td>\n",
       "      <td>0.857299</td>\n",
       "      <td>0.797994</td>\n",
       "      <td>0.374829</td>\n",
       "      <td>0.370199</td>\n",
       "      <td>...</td>\n",
       "      <td>0.999736</td>\n",
       "      <td>0.987967</td>\n",
       "      <td>0.852900</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.843829</td>\n",
       "      <td>0.930905</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.370687</td>\n",
       "      <td>0.371392</td>\n",
       "      <td>0.133333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>41214 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           1         2         3    4         5         6         7   \\\n",
       "1    0.002058  0.476271  0.831354  1.0  0.947018  0.942122  0.921016   \n",
       "1    0.004115  0.595087  0.736342  0.6  0.891781  0.832717  0.781176   \n",
       "1    0.006173  0.999810  1.000000  1.0  0.857964  0.852921  0.833388   \n",
       "1    0.008230  0.999893  0.997862  1.0  0.857964  0.852859  0.833121   \n",
       "1    0.010288  0.833363  0.997625  1.0  0.866524  0.862037  0.841417   \n",
       "..        ...       ...       ...  ...       ...       ...       ...   \n",
       "248  0.569959  0.999788  0.997743  1.0  0.857964  0.854583  0.848481   \n",
       "248  0.572016  0.476162  0.831948  1.0  0.947018  0.944187  0.929542   \n",
       "248  0.574074  0.833146  0.999169  1.0  0.866524  0.863310  0.852368   \n",
       "248  0.576132  0.476164  0.831354  1.0  0.947018  0.944482  0.926035   \n",
       "248  0.578189  0.833353  0.997862  1.0  0.866524  0.864056  0.857299   \n",
       "\n",
       "           8         9         10  ...        17        18        19  \\\n",
       "1    0.858933  0.639535  0.629338  ...  0.999397  0.981966  0.837152   \n",
       "1    0.721587  0.482216  0.416474  ...  0.848850  0.959333  0.986040   \n",
       "1    0.773635  0.267442  0.263304  ...  0.999468  0.984668  0.852864   \n",
       "1    0.774600  0.267442  0.263304  ...  0.999448  0.985615  0.849542   \n",
       "1    0.781974  0.374829  0.368811  ...  0.999435  0.983046  0.844422   \n",
       "..        ...       ...       ...  ...       ...       ...       ...   \n",
       "248  0.789928  0.267442  0.264692  ...  0.999699  0.989729  0.857103   \n",
       "248  0.882067  0.639535  0.632115  ...  0.999627  0.986584  0.842080   \n",
       "248  0.791187  0.374829  0.370199  ...  0.999757  0.988293  0.853063   \n",
       "248  0.878828  0.639535  0.632115  ...  0.999644  0.986390  0.841672   \n",
       "248  0.797994  0.374829  0.370199  ...  0.999736  0.987967  0.852900   \n",
       "\n",
       "           20        21        22      23        24        25        0   \n",
       "1    0.666667  0.911839  0.973199  1.0000  0.612188  0.616904  0.112821  \n",
       "1    0.666667  0.770781  0.801926  0.8493  0.361622  0.359835  0.112821  \n",
       "1    0.666667  0.826196  0.926298  1.0000  0.261647  0.270103  0.112821  \n",
       "1    0.666667  0.826196  0.926298  1.0000  0.265928  0.261719  0.112821  \n",
       "1    0.666667  0.838791  0.930905  1.0000  0.373961  0.374850  0.112821  \n",
       "..        ...       ...       ...     ...       ...       ...       ...  \n",
       "248  0.666667  0.833753  0.926298  1.0000  0.265173  0.262780  0.133333  \n",
       "248  0.666667  0.921914  0.973199  1.0000  0.612692  0.614718  0.133333  \n",
       "248  0.666667  0.846348  0.930905  1.0000  0.369932  0.370918  0.133333  \n",
       "248  1.000000  0.921914  0.973199  1.0000  0.615462  0.616400  0.133333  \n",
       "248  0.666667  0.843829  0.930905  1.0000  0.370687  0.371392  0.133333  \n",
       "\n",
       "[41214 rows x 26 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#TESTING DATA\n",
    "##load data\n",
    "# data1 = scipy.io.loadmat('electricitypricing.mat')\n",
    "# data = data1.get('data')\n",
    "data1 = pandas.read_csv(\"test_FD004.txt\", sep=' ', lineterminator='\\n', header = None,index_col=0)\n",
    "n_feature = 25\n",
    "RUL = pandas.read_csv(\"RUL_FD004.txt\", sep=' ', lineterminator='\\n', header = None)\n",
    "LYR = RUL[0].values.tolist()\n",
    "d2 = dict(enumerate(LYR,start =1))\n",
    "d3 = pandas.DataFrame.from_dict(d2,orient='index')\n",
    "result = pandas.concat([data1, d3], axis=1,join=\"outer\")\n",
    "data2 = result.dropna(axis='columns')\n",
    "normalised_df = maximum_absolute_scaling(data2)\n",
    "data = normalised_df.to_numpy()\n",
    "print(type(data))\n",
    "normalised_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>...</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>20.0072</td>\n",
       "      <td>0.7000</td>\n",
       "      <td>100.0</td>\n",
       "      <td>491.19</td>\n",
       "      <td>606.67</td>\n",
       "      <td>1481.04</td>\n",
       "      <td>1227.81</td>\n",
       "      <td>9.35</td>\n",
       "      <td>13.60</td>\n",
       "      <td>...</td>\n",
       "      <td>8048.98</td>\n",
       "      <td>9.2229</td>\n",
       "      <td>0.02</td>\n",
       "      <td>362</td>\n",
       "      <td>2324</td>\n",
       "      <td>100.00</td>\n",
       "      <td>24.31</td>\n",
       "      <td>14.7007</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>24.9984</td>\n",
       "      <td>0.6200</td>\n",
       "      <td>60.0</td>\n",
       "      <td>462.54</td>\n",
       "      <td>536.22</td>\n",
       "      <td>1256.17</td>\n",
       "      <td>1031.48</td>\n",
       "      <td>7.05</td>\n",
       "      <td>9.00</td>\n",
       "      <td>...</td>\n",
       "      <td>7863.46</td>\n",
       "      <td>10.8632</td>\n",
       "      <td>0.02</td>\n",
       "      <td>306</td>\n",
       "      <td>1915</td>\n",
       "      <td>84.93</td>\n",
       "      <td>14.36</td>\n",
       "      <td>8.5748</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>42.0000</td>\n",
       "      <td>0.8420</td>\n",
       "      <td>100.0</td>\n",
       "      <td>445.00</td>\n",
       "      <td>549.23</td>\n",
       "      <td>1340.13</td>\n",
       "      <td>1105.88</td>\n",
       "      <td>3.91</td>\n",
       "      <td>5.69</td>\n",
       "      <td>...</td>\n",
       "      <td>8071.13</td>\n",
       "      <td>9.3960</td>\n",
       "      <td>0.02</td>\n",
       "      <td>328</td>\n",
       "      <td>2212</td>\n",
       "      <td>100.00</td>\n",
       "      <td>10.39</td>\n",
       "      <td>6.4365</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>42.0035</td>\n",
       "      <td>0.8402</td>\n",
       "      <td>100.0</td>\n",
       "      <td>445.00</td>\n",
       "      <td>549.19</td>\n",
       "      <td>1339.70</td>\n",
       "      <td>1107.26</td>\n",
       "      <td>3.91</td>\n",
       "      <td>5.69</td>\n",
       "      <td>...</td>\n",
       "      <td>8078.89</td>\n",
       "      <td>9.3594</td>\n",
       "      <td>0.02</td>\n",
       "      <td>328</td>\n",
       "      <td>2212</td>\n",
       "      <td>100.00</td>\n",
       "      <td>10.56</td>\n",
       "      <td>6.2367</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>35.0079</td>\n",
       "      <td>0.8400</td>\n",
       "      <td>100.0</td>\n",
       "      <td>449.44</td>\n",
       "      <td>555.10</td>\n",
       "      <td>1353.04</td>\n",
       "      <td>1117.80</td>\n",
       "      <td>5.48</td>\n",
       "      <td>7.97</td>\n",
       "      <td>...</td>\n",
       "      <td>8057.83</td>\n",
       "      <td>9.3030</td>\n",
       "      <td>0.02</td>\n",
       "      <td>333</td>\n",
       "      <td>2223</td>\n",
       "      <td>100.00</td>\n",
       "      <td>14.85</td>\n",
       "      <td>8.9326</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>248</td>\n",
       "      <td>277</td>\n",
       "      <td>41.9991</td>\n",
       "      <td>0.8401</td>\n",
       "      <td>100.0</td>\n",
       "      <td>445.00</td>\n",
       "      <td>550.30</td>\n",
       "      <td>1364.40</td>\n",
       "      <td>1129.17</td>\n",
       "      <td>3.91</td>\n",
       "      <td>5.72</td>\n",
       "      <td>...</td>\n",
       "      <td>8112.61</td>\n",
       "      <td>9.4427</td>\n",
       "      <td>0.02</td>\n",
       "      <td>331</td>\n",
       "      <td>2212</td>\n",
       "      <td>100.00</td>\n",
       "      <td>10.53</td>\n",
       "      <td>6.2620</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>248</td>\n",
       "      <td>278</td>\n",
       "      <td>20.0026</td>\n",
       "      <td>0.7005</td>\n",
       "      <td>100.0</td>\n",
       "      <td>491.19</td>\n",
       "      <td>608.00</td>\n",
       "      <td>1494.75</td>\n",
       "      <td>1260.88</td>\n",
       "      <td>9.35</td>\n",
       "      <td>13.66</td>\n",
       "      <td>...</td>\n",
       "      <td>8086.83</td>\n",
       "      <td>9.2772</td>\n",
       "      <td>0.02</td>\n",
       "      <td>366</td>\n",
       "      <td>2324</td>\n",
       "      <td>100.00</td>\n",
       "      <td>24.33</td>\n",
       "      <td>14.6486</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>248</td>\n",
       "      <td>279</td>\n",
       "      <td>34.9988</td>\n",
       "      <td>0.8413</td>\n",
       "      <td>100.0</td>\n",
       "      <td>449.44</td>\n",
       "      <td>555.92</td>\n",
       "      <td>1370.65</td>\n",
       "      <td>1130.97</td>\n",
       "      <td>5.48</td>\n",
       "      <td>8.00</td>\n",
       "      <td>...</td>\n",
       "      <td>8100.84</td>\n",
       "      <td>9.3982</td>\n",
       "      <td>0.02</td>\n",
       "      <td>336</td>\n",
       "      <td>2223</td>\n",
       "      <td>100.00</td>\n",
       "      <td>14.69</td>\n",
       "      <td>8.8389</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>248</td>\n",
       "      <td>280</td>\n",
       "      <td>20.0027</td>\n",
       "      <td>0.7000</td>\n",
       "      <td>100.0</td>\n",
       "      <td>491.19</td>\n",
       "      <td>608.19</td>\n",
       "      <td>1489.11</td>\n",
       "      <td>1256.25</td>\n",
       "      <td>9.35</td>\n",
       "      <td>13.66</td>\n",
       "      <td>...</td>\n",
       "      <td>8085.24</td>\n",
       "      <td>9.2727</td>\n",
       "      <td>0.03</td>\n",
       "      <td>366</td>\n",
       "      <td>2324</td>\n",
       "      <td>100.00</td>\n",
       "      <td>24.44</td>\n",
       "      <td>14.6887</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>248</td>\n",
       "      <td>281</td>\n",
       "      <td>35.0075</td>\n",
       "      <td>0.8402</td>\n",
       "      <td>100.0</td>\n",
       "      <td>449.44</td>\n",
       "      <td>556.40</td>\n",
       "      <td>1378.58</td>\n",
       "      <td>1140.70</td>\n",
       "      <td>5.48</td>\n",
       "      <td>8.00</td>\n",
       "      <td>...</td>\n",
       "      <td>8098.17</td>\n",
       "      <td>9.3964</td>\n",
       "      <td>0.02</td>\n",
       "      <td>335</td>\n",
       "      <td>2223</td>\n",
       "      <td>100.00</td>\n",
       "      <td>14.72</td>\n",
       "      <td>8.8502</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>41214 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      1        2       3      4       5       6        7        8     9   \\\n",
       "0                                                                          \n",
       "1      1  20.0072  0.7000  100.0  491.19  606.67  1481.04  1227.81  9.35   \n",
       "1      2  24.9984  0.6200   60.0  462.54  536.22  1256.17  1031.48  7.05   \n",
       "1      3  42.0000  0.8420  100.0  445.00  549.23  1340.13  1105.88  3.91   \n",
       "1      4  42.0035  0.8402  100.0  445.00  549.19  1339.70  1107.26  3.91   \n",
       "1      5  35.0079  0.8400  100.0  449.44  555.10  1353.04  1117.80  5.48   \n",
       "..   ...      ...     ...    ...     ...     ...      ...      ...   ...   \n",
       "248  277  41.9991  0.8401  100.0  445.00  550.30  1364.40  1129.17  3.91   \n",
       "248  278  20.0026  0.7005  100.0  491.19  608.00  1494.75  1260.88  9.35   \n",
       "248  279  34.9988  0.8413  100.0  449.44  555.92  1370.65  1130.97  5.48   \n",
       "248  280  20.0027  0.7000  100.0  491.19  608.19  1489.11  1256.25  9.35   \n",
       "248  281  35.0075  0.8402  100.0  449.44  556.40  1378.58  1140.70  5.48   \n",
       "\n",
       "        10  ...       18       19    20   21    22      23     24       25  \\\n",
       "0           ...                                                              \n",
       "1    13.60  ...  8048.98   9.2229  0.02  362  2324  100.00  24.31  14.7007   \n",
       "1     9.00  ...  7863.46  10.8632  0.02  306  1915   84.93  14.36   8.5748   \n",
       "1     5.69  ...  8071.13   9.3960  0.02  328  2212  100.00  10.39   6.4365   \n",
       "1     5.69  ...  8078.89   9.3594  0.02  328  2212  100.00  10.56   6.2367   \n",
       "1     7.97  ...  8057.83   9.3030  0.02  333  2223  100.00  14.85   8.9326   \n",
       "..     ...  ...      ...      ...   ...  ...   ...     ...    ...      ...   \n",
       "248   5.72  ...  8112.61   9.4427  0.02  331  2212  100.00  10.53   6.2620   \n",
       "248  13.66  ...  8086.83   9.2772  0.02  366  2324  100.00  24.33  14.6486   \n",
       "248   8.00  ...  8100.84   9.3982  0.02  336  2223  100.00  14.69   8.8389   \n",
       "248  13.66  ...  8085.24   9.2727  0.03  366  2324  100.00  24.44  14.6887   \n",
       "248   8.00  ...  8098.17   9.3964  0.02  335  2223  100.00  14.72   8.8502   \n",
       "\n",
       "     26  27  \n",
       "0            \n",
       "1   NaN NaN  \n",
       "1   NaN NaN  \n",
       "1   NaN NaN  \n",
       "1   NaN NaN  \n",
       "1   NaN NaN  \n",
       "..   ..  ..  \n",
       "248 NaN NaN  \n",
       "248 NaN NaN  \n",
       "248 NaN NaN  \n",
       "248 NaN NaN  \n",
       "248 NaN NaN  \n",
       "\n",
       "[41214 rows x 27 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class nn(object):\n",
    "    def __init__(self, layer):\n",
    "        self.size = layer\n",
    "        self.n = len(layer)\n",
    "        self.hiddenLayer = self.n - 2\n",
    "        self.activation_func = \"sigm\"\n",
    "        self.learningRate = 0.01\n",
    "        self.momentum = 0.95\n",
    "        self.outputConnect = 1\n",
    "        self.output = \"linear\"\n",
    "        \n",
    "        # initiate weights and weight momentum for hidden layer\n",
    "        self.W = {}\n",
    "        self.vW = {}\n",
    "        self.dW = {}\n",
    "        self.c = {}\n",
    "        #self.W = self.vW  = self.dW= self.c = {} \n",
    "        for i in range(1, self.n-1):\n",
    "            self.W[i] = np.random.normal(0, np.sqrt(2/(self.size[i-1]+1)), size=(self.size[i],self.size[i - 1]+1))\n",
    "            self.vW[i] = np.zeros(self.W[i].shape)\n",
    "            self.dW[i] = np.zeros(self.W[i].shape)\n",
    "            self.c[i] =  np.random.normal(0, np.sqrt(2/(self.size[i-1]+1)), size=(self.size[i - 1],1))\n",
    "        \n",
    "        #initiate weights and weight momentum for output layer\n",
    "        self.Ws = {}\n",
    "        self.vWs = {}\n",
    "        self.dWs =  {}\n",
    "        self.beta = {}\n",
    "        self.betaOld = {}\n",
    "        self.p= {}  \n",
    "        if self.outputConnect == 1:\n",
    "            for i in range(1,(self.hiddenLayer + 1)):\n",
    "                self.Ws[i] = np.random.normal(0, np.sqrt(2/((self.W[i].shape[0])+1)), size=(self.size[len(self.size)-1],self.size[i]+1))\n",
    "                self.vWs[i] = np.zeros(self.Ws[i].shape)\n",
    "                self.dWs[i] = np.zeros(self.Ws[i].shape)         \n",
    "                self.beta[i] = 1\n",
    "                self.betaOld[i] = 1\n",
    "                self.p[i]= 1 \n",
    "        else:\n",
    "            i = 1\n",
    "            self.Ws[i] = np.random.normal(0, np.sqrt(2/((self.W[i-1].shape[0])+1)), size=(self.size[len(self.size)-1],self.size[len(self.size)-2]+1))\n",
    "            self.vWs[i] = np.zeros(self.Ws[i].shape)\n",
    "            self.dWs[i] = np.zeros(self.Ws[i].shape) \n",
    "            \n",
    "        #initiate later used variable\n",
    "        self.a = {}\n",
    "        self.aas = {}\n",
    "        self.e = {}\n",
    "        self.L = {}\n",
    "        self.classlabel = {}\n",
    "        self.nop = {}\n",
    "        self.nodes = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "uhuy = nn([8,5,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 6)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uhuy.Ws[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Parameter(object):\n",
    "    def __init__(self, nn,layer,K):\n",
    "        self.nn = nn\n",
    "        self.ev = {}\n",
    "        self.size = layer\n",
    "        self.prune_list       = 0;\n",
    "        self.prune_list_index = [];\n",
    "        self.ev[1] = {'layer': layer, 'kp':0, 'kl':0 ,'miu_x_old':0, 'var_x_old':0, 'kl':0,'K':K, 'cr':0,'node':{},\n",
    "                     'BIAS2':{}, 'VAR':{}, 'miu_NS_old':0, 'var_NS_old':0, 'miu_NHS_old':0, 'var_NHS_old':0,\n",
    "                     'miumin_NS':[], 'miumin_NHS':[], 'stdmin_NS':[], 'stdmin_NHS':[]}\n",
    "        self.Loss = {}\n",
    "        self.cr = {}\n",
    "        self.wl = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Performance(object):\n",
    "    def __init__(self):\n",
    "        self.update_time = 0\n",
    "        self.ev = {}\n",
    "        self.test_time = 0\n",
    "        self.classification_time    = 0;\n",
    "        self.layer = 0;\n",
    "        self.ev[1] = {'f_measure': 0, 'g_mean':0, 'recall':0, 'precision':0}\n",
    "        self.LayerWeight = 0\n",
    "        self.meanode = []\n",
    "        self.stdnode = []\n",
    "        self.NumberOfParameters = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def sigmoid_array(x):                                        \n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def relu_array(x):\n",
    "    return abs(x) * (x > 0)\n",
    "\n",
    "def softmax_array(x):\n",
    "    list_re = []\n",
    "    for arr in x:\n",
    "        e_x = np.exp(arr - np.max(arr))\n",
    "        result = e_x / e_x.sum()\n",
    "        list_re.append(result)\n",
    "    return np.array(list_re)  \n",
    "\n",
    "def argmax_array(x):\n",
    "    list_re = []\n",
    "    for arr in x:\n",
    "        result = [arr.argmax()]\n",
    "        list_re.append(result)\n",
    "    return np.array(list_re)  \n",
    "\n",
    "def valuemax_array(x):\n",
    "    list_re = []\n",
    "    for arr in x:\n",
    "        result = [arr.max()]\n",
    "        list_re.append(result)\n",
    "    return np.array(list_re)  \n",
    "\n",
    "def update_beta(x):\n",
    "    result = {}\n",
    "    sum_val = sum(x.values())\n",
    "    for k, v in x.items():\n",
    "        result[k] = v/sum_val        \n",
    "    return result\n",
    "\n",
    "def checkbeta(dict1):\n",
    "    count = 0\n",
    "    for i in dict1.values():\n",
    "        if(i != 0):\n",
    "            count += 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testing(nn, x, y, ev):\n",
    "    nop = []\n",
    "    #feedforward\n",
    "    nn = netfeedforward(nn,x,y)\n",
    "    (row,m2) = y.shape\n",
    "    decreasingFactor = 0.001\n",
    "    #obtain true class\n",
    "    act = argmax_array(y)\n",
    "    for i in range(1,nn.hiddenLayer+1):\n",
    "        nn.classlabel[i] = {}\n",
    "    nn.sigma = np.zeros((row,m2))\n",
    "    for t in range(0,row):\n",
    "        for i in range(1,nn.hiddenLayer+1):\n",
    "            if(nn.beta[i] != 0):\n",
    "                #obtain predicted label, note : layer weight betaOld is fixed\n",
    "                nn.sigma[t] = nn.sigma[t] + (nn.aas[i][t] * nn.betaOld[i])               \n",
    "                nn.classlabel[i][t] = nn.aas[i][t].argmax()\n",
    "                compare = act[t][0] - nn.classlabel[i][t]\n",
    "                #train the weighted voting\n",
    "                if(compare != 0):\n",
    "                    nn.beta[i] = max(nn.beta[i] * nn.p[i], decreasingFactor)\n",
    "                    nn.p[i] = max(nn.p[i] - decreasingFactor , decreasingFactor)\n",
    "                elif(compare == 0):\n",
    "                    nn.beta[i] = min(nn.beta[i] * (1+nn.p[i]), 1)\n",
    "                    nn.p[i] = min(nn.p[i] + decreasingFactor , 1)\n",
    "            #last element on data chunk        \n",
    "\n",
    "    #calculate number of parameter\n",
    "    if(nn.beta[i] != 0):\n",
    "        (c,d) = nn.Ws[i].shape\n",
    "        vw = 1\n",
    "    else:\n",
    "        c = 0\n",
    "        d = 0\n",
    "        vw = 0\n",
    "    (a,b) = nn.W[i].shape\n",
    "    nop.append(a*b + c*d + vw) # number of parameters\n",
    "\n",
    "    #calculate the number of node in each hidden layer\n",
    "#     print(ev[1])\n",
    "#     nn.nodes[0][nn.t] = ev[1]['K']\n",
    "    #print(nn.beta)\n",
    "    nn.beta = update_beta(nn.beta);\n",
    "    \n",
    "    nn.nop[nn.t] = np.sum(nop)\n",
    "    nn.mnop = [np.array(list(nn.nop.values())).mean(),np.array(list(nn.nop.values())).std()]\n",
    "    \n",
    "    #update voting weight\n",
    "    nn.betaOld = nn.beta.copy()\n",
    "    nn.index = max(nn.beta, key=nn.beta.get)\n",
    "    \n",
    "    \n",
    "    #calculate classification rate\n",
    "    [raw_out, out] = [valuemax_array(nn.sigma), argmax_array(nn.sigma)]\n",
    "    nn.wrongClass = (np.where(out != act)[0]).reshape(-1,1)\n",
    "    nn.cr = 1 - (nn.wrongClass.shape[0]/row)\n",
    "    nn.residual_error = 1 - raw_out\n",
    "    nn.actualLabel = out\n",
    "    nn.act = act\n",
    "    return [nn]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "def netfeedforward(nn,x,y):\n",
    "    n = nn.n\n",
    "    m = x.shape[0]\n",
    "    ones = np.ones(shape= (m,1))\n",
    "    x = np.append(ones, x, axis=1 ) #append columns of 1 in matrix for bias\n",
    "    nn.a[1] = x\n",
    "    #feedforward from input layer through all the hidden layer\n",
    "    for i in range(2,n):\n",
    "        if(nn.activation_func == 'sigm'):\n",
    "            nn.a[i] = sigmoid_array(np.matmul(nn.a[i-1],nn.W[i-1].T))\n",
    "        elif(nn.activation_func == 'relu'):\n",
    "            nn.a[i] = relu_array(np.matmul(nn.a[i-1],nn.W[i-1].T))\n",
    "        ones = np.ones(shape= (m,1))\n",
    "        nn.a[i] = np.append(ones, nn.a[i], axis=1 )\n",
    "    \n",
    "    #propagate to the output layer\n",
    "    for i in range(1,nn.hiddenLayer+1):\n",
    "        if nn.beta[i] != 0:\n",
    "            if(nn.output == 'sigm'):\n",
    "                nn.aas[i] = sigmoid_array(np.matmul(nn.a[i+1], nn.Ws[i].T))\n",
    "            elif(nn.output == 'linear'):\n",
    "                nn.aas[i] = np.matmul(nn.a[i+1], nn.Ws[i].T)   \n",
    "            elif(nn.output == 'softmax'):\n",
    "                nn.aas[i] = softmax_array(np.matmul(nn.a[i+1],nn.Ws[i].T)) \n",
    "            \n",
    "            \n",
    "            #calculate error\n",
    "            nn.e[i] = y - nn.aas[i]\n",
    "#            nn.L[i] = mean_squared_error(y, nn.aas[i])\n",
    "               \n",
    "            #calculate loss function\n",
    "            if(nn.output == 'sigm' or nn.output == 'linear'):\n",
    "                nn.L[i] = 0.5 * (np.sum(np.sum(nn.e[i]**2))) / m\n",
    "            elif(nn.output == 'softmax'):\n",
    "                nn.L[i] = -np.sum(np.sum(y * np.log(nn.aas[i]))) / m\n",
    "        \n",
    "            #print(nn.L) \n",
    "        \n",
    "    return nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#net configuration while training\n",
    "def netconfigtrain(layer):\n",
    "    net = nn(layer)\n",
    "    net.layer = layer\n",
    "    net.n = len(layer)\n",
    "    net.activation_func = \"sigm\"\n",
    "    net.learningRate = 0.01\n",
    "    net.momentum = 0.95\n",
    "    net.output = \"linear\"\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#calculate probit function\n",
    "def probit(miu,std):\n",
    "    p = (miu / (1+ np.pi * (std**2)/8) ** 0.5)\n",
    "    return p\n",
    "\n",
    "#calculate recursive mean and standard deviation\n",
    "def meanstditer(miu_old, var_old,x,k):\n",
    "    miu = miu_old + (x - miu_old) / k\n",
    "    var = var_old + (x - miu_old) * (x-miu)\n",
    "    std = np.sqrt(var/k)\n",
    "  \n",
    "    if(len(miu.shape) != 2):\n",
    "        #print('len !=2')\n",
    "        #print([np.array([miu]),np.array([std]),np.array([var])])\n",
    "        return [np.array([miu]),np.array([std]),np.array([var])]\n",
    "    else:\n",
    "        return [miu,std,var]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def netFeedForwardWinner(nn,x,y):\n",
    "    x = np.array([x])\n",
    "    y = np.array([y])\n",
    "    n = nn.n\n",
    "    m = x.shape[0]\n",
    "    nn.a[1] = x\n",
    "   \n",
    "    #feedforward form input layer through all the hidden layer \n",
    "    for i in range(2,n):\n",
    "        if(nn.activation_func == 'sigm'):\n",
    "            nn.a[i] = sigmoid_array(np.matmul(nn.a[i-1],nn.W[i-1].T))\n",
    "        elif(nn.activation_func == 'relu'):\n",
    "            nn.a[i] = relu_array(np.matmul(nn.a[i-1],nn.W[i-1].T))\n",
    "        ones = np.ones(shape= (m,1))\n",
    "        nn.a[i] = np.append(ones, nn.a[i], axis=1 )\n",
    "  \n",
    "    #propagate to the output layer\n",
    "    if(nn.output == 'sigm'):\n",
    "        nn.a[n] = sigmoid_array(np.matmul(nn.a[n-1],nn.W[n-1].T))\n",
    "    elif(nn.output == 'linear'):\n",
    "        nn.a[n] = np.matmul(nn.a[n-1],nn.W[n-1].T)   \n",
    "    elif(nn.output == 'softmax'):\n",
    "        nn.a[n] = softmax_array(np.matmul(nn.a[n-1], nn.W[n-1].T)) \n",
    "            \n",
    "    #calculate error\n",
    "    #mean_squared_error(y, nn.a[n], squared=False)\n",
    "    nn.e[1] = y - nn.a[n]\n",
    "    return nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate backpropagation\n",
    "def lossBackward(nn):\n",
    "    n = nn.n\n",
    "    d = {}\n",
    "    if(nn.output == 'sigm'):\n",
    "        d[n] = - nn.e[1] * (nn.a[n] * (1 - nn.a[n]))      \n",
    "    elif(nn.output == 'linear' or nn.output == 'softmax'):\n",
    "        d[n] = -1 * nn.e[1]\n",
    "    #activation backward\n",
    "    for i in range(n-1, 1, -1):\n",
    "        if(nn.activation_func == 'sigm'):\n",
    "            d_act = nn.a[i] * (1 - nn.a[i])\n",
    "        elif(nn.activation_func == 'tanh_opt'):\n",
    "            d_act = 1.7159 * 2/3 * (1 - 1 / (1.7159**2) * (nn.a[i]**2))   \n",
    "        elif(nn.activation_func == 'relu'):\n",
    "            d_act = np.zeros((1,len(nn.a[i])))\n",
    "            for i in range(len(d_act)):\n",
    "                print(type((nn.a[i + 1])))\n",
    "                if(nn.a[i + 1].all()>0):\n",
    "                    d_act[i] = 0             \n",
    "        if(i+1 == n):\n",
    "            d[i] = (np.matmul(d[i+1], nn.W[i]) * d_act)\n",
    "        else:\n",
    "            d[i] = (np.matmul(d[i+1][1:], nn.W[i]) * d_act)\n",
    "\n",
    "    for i in range(1, n): \n",
    "        if(i+1 == n):\n",
    "            nn.dW[i] = np.matmul(d[i+1].T, nn.a[i]) \n",
    "        else: \n",
    "            nn.dW[i] = np.matmul(d[i+1][:,1:].T, nn.a[i])   \n",
    "    return nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#update the weight\n",
    "def optimizerStep(nn):\n",
    "    for i in range(1, nn.n):\n",
    "        dW = nn.dW[i]\n",
    "        dW = nn.learningRate * dW       \n",
    "        if(nn.momentum > 0):\n",
    "            nn.vW[i] = nn.momentum * nn.vW[i] + dW\n",
    "            dW = nn.vW[i]\n",
    "        nn.W[i] = nn.W[i] - dW\n",
    "    return nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(parameter,y):\n",
    "    bb = parameter.nn.W[parameter.nn.index].shape[1]\n",
    "    grow = 0\n",
    "    prune=0\n",
    "    #initialize performance matrix\n",
    "    ly          = parameter.nn.index\n",
    "    kp          = parameter.ev[1]['kp']\n",
    "    miu_x_old   = parameter.ev[1]['miu_x_old']\n",
    "    var_x_old   = parameter.ev[1]['var_x_old']\n",
    "    kl          = parameter.ev[ly]['kl']\n",
    "    K           = parameter.ev[ly]['K']\n",
    "    node        = parameter.ev[ly]['node']\n",
    "    BIAS2       = parameter.ev[ly]['BIAS2']\n",
    "    VAR         = parameter.ev[ly]['VAR']\n",
    "    miu_NS_old  = parameter.ev[ly]['miu_NS_old']\n",
    "    var_NS_old  = parameter.ev[ly]['var_NS_old']\n",
    "    miu_NHS_old = parameter.ev[ly]['miu_NHS_old']\n",
    "    var_NHS_old = parameter.ev[ly]['var_NHS_old']\n",
    "    miumin_NS   = parameter.ev[ly]['miumin_NS']\n",
    "    miumin_NHS  = parameter.ev[ly]['miumin_NHS']\n",
    "    stdmin_NS   = parameter.ev[ly]['stdmin_NS']\n",
    "    stdmin_NHS  = parameter.ev[ly]['stdmin_NHS']\n",
    "    \n",
    "    \n",
    "    #initiate training model\n",
    "    net = netconfigtrain([1,1,1])\n",
    "    #substitute the weight to be trained to training model\n",
    "    net.activation_func = parameter.nn.activation_func\n",
    "    net.W[1]  = parameter.nn.W[ly]\n",
    "    net.vW[1] = parameter.nn.vW[ly]\n",
    "    net.dW[1] = parameter.nn.dW[ly]\n",
    "    net.W[2]  = parameter.nn.Ws[ly]\n",
    "    net.vW[2] = parameter.nn.vWs[ly]\n",
    "    net.dW[2] = parameter.nn.dWs[ly]\n",
    "    \n",
    "    #load data in shuffled\n",
    "    x = parameter.nn.a[ly]\n",
    "    (N,I) = x.shape\n",
    "    kk = np.random.permutation(N)\n",
    "    x = x[kk]\n",
    "    y = y[kk]\n",
    "    \n",
    "    #xavier initialization\n",
    "    if(ly>1):\n",
    "        n_in = parameter.ev[ly-1]['K']\n",
    "    else:\n",
    "        n_in = parameter.nn.size[0]\n",
    "\n",
    "    miuNS       = np.zeros((N,1))\n",
    "    miuminNS       = np.zeros((N,1))\n",
    "    miuNHS       = np.zeros((N,1))\n",
    "    miuminNHS       = np.zeros((N,1))\n",
    "\n",
    "    \n",
    "    #main loop, train the model\n",
    "    for k in range(1, N+1):\n",
    "        kp = kp+1\n",
    "        kl = kl+1\n",
    "        #incremental calculation of x_tail mean and variance\n",
    "        [miu_x, std_x, var_x] = meanstditer(miu_x_old,var_x_old,parameter.nn.a[1][k-1],kp)\n",
    "        miu_x_old = miu_x\n",
    "        var_x_old = var_x\n",
    "\n",
    "        #expectation of z\n",
    "        py = probit(miu_x,std_x).T\n",
    "      \n",
    "        for ii in range(1, parameter.nn.index+1):\n",
    "            if(ii == parameter.nn.index):\n",
    "                py = sigmoid_array(np.matmul(net.W[1],py))\n",
    "            else:\n",
    "                py = sigmoid_array(np.matmul(parameter.nn.W[ii],py))\n",
    "            py = np.append(np.array([[1]]),py, axis = 0)\n",
    "            if(ii == 1):\n",
    "                Ey2 = py**2\n",
    "\n",
    "        Ey = py\n",
    "        Ez = np.matmul(net.W[2], Ey) \n",
    "        Ez = softmax_array(Ez.T).T  # E[output]\n",
    "        \n",
    "        #expectation of output\n",
    "        if(parameter.nn.hiddenLayer>1):\n",
    "            py = Ey2\n",
    "            for ii in range(2, parameter.nn.index+1):\n",
    "                if(ii==parameter.nn.index):\n",
    "                    py = sigmoid_array(np.matmul(net.W[1],py))\n",
    "                else:\n",
    "                    py = sigmoid_array(np.matmul(parameter.nn.W[ii],py))\n",
    "                py = np.append(np.array([[1]]),py, axis = 0)\n",
    "            Ey2 = py\n",
    "        Ez2 = np.matmul(net.W[2], Ey2) \n",
    "        Ez2 = softmax_array(Ez2.T).T\n",
    "          \n",
    "            \n",
    "        #Network mean calculation  \n",
    "        bias2 = (Ez - np.transpose(y[k-1, :])) ** 2\n",
    "        ns = bias2\n",
    "        NS = np.linalg.norm(ns, 'fro')\n",
    "\n",
    "        #Incremental calculation of NS mean and variance\n",
    "        [miu_NS,std_NS,var_NS] = meanstditer(miu_NS_old,var_NS_old,NS,kp)\n",
    "        miu_NS_old = miu_NS\n",
    "        var_NS_old = var_NS\n",
    "        miustd_NS  = miu_NS + std_NS\n",
    "        miuNS[k-1] = miu_NS\n",
    "        \n",
    "        if(kl <= 1 or grow ==1):\n",
    "            miumin_NS = miu_NS\n",
    "            stdmin_NS = std_NS\n",
    "        else:\n",
    "            if(miu_NS < miumin_NS):\n",
    "                miumin_NS = miu_NS\n",
    "                stdmin_NS = std_NS\n",
    "            if(std_NS < stdmin_NS):\n",
    "                stdmin_NS = std_NS\n",
    "        miuminNS[k-1] = miumin_NS\n",
    "        miustdmin_NS  = miumin_NS + (1.3*np.exp(-NS)+0.7)*stdmin_NS\n",
    "        BIAS2[kl] = miu_NS\n",
    "        \n",
    "        #growing hidden unit\n",
    "        if(miustd_NS >= miustdmin_NS and kl>1):\n",
    "            grow = 1\n",
    "            K=K+1\n",
    "            print('The new node no {} is FORMED around sample {}'.format(K, k))\n",
    "            node[kp] = K\n",
    "            W_app = np.random.normal(0, np.sqrt(2/(n_in+1)), size=(1, bb))\n",
    "            net.W[1] = np.append(net.W[1],W_app, axis = 0)          \n",
    "            net.vW[1] = np.append(net.vW[1],np.zeros((1,bb)), axis = 0)\n",
    "            net.dW[1] = np.append(net.dW[1],np.zeros((1,bb)), axis = 0)\n",
    "            \n",
    "            W2_app = np.random.normal(0, np.sqrt(2/(K+1)), size=(parameter.nn.size[-1], 1))\n",
    "            net.W[2] = np.append(net.W[2],W2_app, axis = 1)\n",
    "            net.vW[2] = np.append(net.vW[2],np.zeros((parameter.nn.size[-1], 1)), axis = 1)\n",
    "            net.dW[2] = np.append(net.dW[2],np.zeros((parameter.nn.size[-1], 1)), axis = 1)\n",
    "            \n",
    "            if(ly<parameter.nn.hiddenLayer):\n",
    "                wNext = parameter.nn.W[ly+1].shape[0]\n",
    "                parameter.nn.W[ly+1]  = np.append(parameter.nn.W[ly+1],np.random.normal(0, np.sqrt(2/(K+1)), size=(wNext, 1)), axis = 1)\n",
    "                parameter.nn.vW[ly+1] = np.append(parameter.nn.vW[ly+1],np.zeros((wNext, 1)), axis = 1) \n",
    "                parameter.nn.dW[ly+1] = np.append(parameter.nn.dW[ly+1],np.zeros((wNext, 1)), axis = 1)         \n",
    "        else:\n",
    "            grow = 0\n",
    "            node[kp] = K\n",
    "        #Network variance calculation\n",
    "#         ho = np.array([[0.7498],[0.2502]])\n",
    "#         he = np.array([[0.7856],[0.2144]])\n",
    "#         uy = ho-he**2\n",
    "#         ey = np.linalg.norm(uy, 'fro')\n",
    "       \n",
    "        var = Ez2 - Ez**2\n",
    "        NHS = np.linalg.norm(var, 'fro')\n",
    "        \n",
    "        # Incremental calculation of NHS mean and variance\n",
    "        [miu_NHS,std_NHS,var_NHS] = meanstditer(miu_NHS_old,var_NHS_old,NHS,kp)\n",
    "        miu_NHS_old = miu_NHS\n",
    "        var_NHS_old = var_NHS\n",
    "        miustd_NHS  = miu_NHS + std_NHS\n",
    "        miuNHS[k-1] = miu_NHS\n",
    "        \n",
    "        if(kl <= I+1 or prune ==1):\n",
    "            miumin_NHS = miu_NHS\n",
    "            stdmin_NHS = std_NHS\n",
    "        else:\n",
    "            if(miu_NHS < miumin_NHS):\n",
    "                miumin_NHS = miu_NHS\n",
    "            if(std_NHS < stdmin_NHS):\n",
    "                stdmin_NHS = std_NHS\n",
    "        miuminNHS[k-1] = miumin_NHS\n",
    "        miustdmin_NHS  = miumin_NHS + (2.6*np.exp(-NHS)+1.4)*stdmin_NHS\n",
    "        VAR[kl] = miu_NHS\n",
    "        \n",
    "        # Pruning hidden unit\n",
    "        if(grow == 0 and K>1 and miustd_NHS >= miustdmin_NHS and kl>I+1):\n",
    "            HS = Ey[1:]\n",
    "            BB = np.argmin(HS)\n",
    "            print('The node no {} is PRUNED around sample {}'.format(BB+1, k))\n",
    "            prune = 1\n",
    "            K = K-1\n",
    "            node[kp] = K\n",
    "            net.W[1] = np.delete(net.W[1],BB,axis =0)\n",
    "            net.vW[1] = np.delete(net.vW[1],BB,axis =0)\n",
    "            net.dW[1] = np.delete(net.dW[1],BB,axis =0)\n",
    "            \n",
    "            net.W[2] = np.delete(net.W[2],BB+1,axis =1)\n",
    "            net.vW[2] = np.delete(net.vW[2],BB+1,axis =1)\n",
    "            net.dW[2] = np.delete(net.dW[2],BB+1,axis =1)\n",
    "            if(ly<parameter.nn.hiddenLayer):\n",
    "                parameter.nn.W[ly+1]  = np.delete(parameter.nn.W[ly+1],BB+1,axis =1)\n",
    "                parameter.nn.vW[ly+1] = np.delete(parameter.nn.vW[ly+1],BB+1,axis =1)\n",
    "                parameter.nn.dW[ly+1] = np.delete(parameter.nn.dW[ly+1],BB+1,axis =1)            \n",
    "        else:\n",
    "            prune = 0\n",
    "            node[kp] = K\n",
    "        #feedforward\n",
    "        net = netFeedForwardWinner(net, x[k-1],y[k-1])\n",
    "        \n",
    "        #optimise parameter\n",
    "        net = lossBackward(net)\n",
    "        net = optimizerStep(net)\n",
    "        \n",
    "    #iterative learning\n",
    "    ##unused\n",
    "#     if nEpoch > 1\n",
    "#     for iEpoch in range(1,nEpoch)\n",
    "#         kk = randperm(nData)\n",
    "#         x = x[kk,:]\n",
    "#         y = y[kk,:]\n",
    "#         for datum in range(nData)\n",
    "#             %% feedforward\n",
    "#             net = netFeedForwardWinner(net, x(iData,:), y(iData,:));\n",
    "            \n",
    "#             %% optimize the parameters\n",
    "#             net = lossBackward(net);\n",
    "#             net = optimizerStep(net);\n",
    "#         end\n",
    "    \n",
    "    # subsitutte weight back to main model    \n",
    "    parameter.nn.W[ly] = net.W[1]\n",
    "    parameter.nn.Ws[ly] = net.W[2]\n",
    "    \n",
    "    #reset momentum and grandient\n",
    "    parameter.nn.vW[ly] = net.vW[1] * 0\n",
    "    parameter.nn.dW[ly] = net.dW[1] * 0\n",
    "    parameter.nn.vWs[ly] = net.vW[2] * 0\n",
    "    parameter.nn.dWs[ly] = net.dW[2] *0\n",
    "    \n",
    "    #substitute the recursive calculation\n",
    "    parameter.ev[1]['kp']           = kp\n",
    "    parameter.ev[1]['miu_x_old']    = miu_x_old\n",
    "    parameter.ev[1]['var_x_old']    = var_x_old\n",
    "    parameter.ev[ly]['kl']          = kl\n",
    "    parameter.ev[ly]['K']           = K\n",
    "    parameter.ev[ly]['node']        = node\n",
    "    parameter.ev[ly]['BIAS2']       = BIAS2\n",
    "    parameter.ev[ly]['VAR']         = VAR\n",
    "    parameter.ev[ly]['miu_NS_old']  = miu_NS_old\n",
    "    parameter.ev[ly]['var_NS_old']  = var_NS_old\n",
    "    parameter.ev[ly]['miu_NHS_old'] = miu_NHS_old\n",
    "    parameter.ev[ly]['var_NHS_old'] = var_NHS_old\n",
    "    parameter.ev[ly]['miumin_NS']   = miumin_NS\n",
    "    parameter.ev[ly]['miumin_NHS']  = miumin_NHS\n",
    "    parameter.ev[ly]['stdmin_NS']   = stdmin_NS\n",
    "    parameter.ev[ly]['stdmin_NHS']  = stdmin_NHS\n",
    "    #print(BIAS2)             \n",
    "    return parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluation method\n",
    "#method for precission, recall, f1 score and gmean\n",
    "def performance_summary(act_raw, out_raw, nclass):\n",
    "    Act = to_one_hot(act_raw, nclass)\n",
    "    Out = to_one_hot(out_raw, nclass)\n",
    "    recall = compute_recall(Act, Out, nclass)\n",
    "    precission = compute_precission(Act, Out, nclass)\n",
    "    f_measure = compute_f_measure(Act, Out, nclass)\n",
    "    g_mean = compute_g_mean(recall, nclass)\n",
    "    return [g_mean, f_measure, precission, recall]\n",
    "\n",
    "def compute_g_mean(recall, nclass):\n",
    "    g_mean = (np.prod(recall))**(1/nclass) \n",
    "    return g_mean\n",
    "\n",
    "def compute_f_measure(Act, Out, nclass):\n",
    "    f_measure = np.zeros((1,nclass))    \n",
    "    for c in range(nclass):\n",
    "        f_measure[0][c] = (2 * (Act[:,c].T.dot(Out[:,c]))) / (np.sum(Out[:,c]) + np.sum(Act[:,c]))\n",
    "    f_measure[np.isnan(f_measure)] = 1\n",
    "    return f_measure\n",
    "\n",
    "def compute_precission(Act, Out, nclass):\n",
    "    precission = np.zeros((1,nclass))    \n",
    "    for c in range(nclass):\n",
    "        precission[0][c] = (Act[:,c].T.dot(Out[:,c])) / np.sum(Out[:,c])\n",
    "    precission[np.isnan(precission)] = 1\n",
    "    return precission\n",
    "    \n",
    "def compute_recall(Act, Out, nclass):\n",
    "    recall = np.zeros((1,nclass))  \n",
    "    for c in range(nclass):\n",
    "        recall[0][c] = (Act[:,c].T.dot(Out[:,c])) / np.sum(Act[:,c])\n",
    "    recall[np.isnan(recall)] = 1\n",
    "    return recall\n",
    "\n",
    "def to_one_hot(x, nclass):\n",
    "    y = np.zeros((x.shape[0],nclass))\n",
    "    for i in range(len(x)):\n",
    "        y[i][x[i]] = 1\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main method\n",
    "def test_ADL(data, n_feature):   \n",
    "    (nData, n_column) = data.shape\n",
    "    M = n_column - n_feature\n",
    "    preq_data = data[:,0:n_feature]\n",
    "    preq_label = data[:,n_feature:]  \n",
    "    chunk_size = 500\n",
    "    no_of_chunk = int(nData/chunk_size)\n",
    "    \n",
    "    #dataProportion = 1 #unused variable\n",
    "    \n",
    "    drift = {}\n",
    "    HL = {}\n",
    "    buffer_x = []\n",
    "    buffer_T = []\n",
    "    tTest = []\n",
    "    tTarget = []\n",
    "    act = []\n",
    "    out = []\n",
    "    #initiate model \n",
    "    K = 1 #initial node\n",
    "    network = nn([n_feature, K, M])\n",
    "    \n",
    "    #initiate node evolving iterative parameters\n",
    "    layer = 1 #initial layer\n",
    "    parameter = Parameter(network, layer,K)\n",
    "    performance = Performance()\n",
    "    \n",
    "    # initiate drift detection parameter\n",
    "    alpha_w = 0.0005;\n",
    "    alpha_d = 0.0001;\n",
    "    alpha   = 0.0001;\n",
    "    \n",
    "    #initiate layer merging iterative parameters\n",
    "    covariance = np.zeros((5,5,M))\n",
    "    covariance_old  = covariance\n",
    "    threshold  = 0.05\n",
    "    \n",
    "    ClassificationRate = {}\n",
    "    start_time = time.time()\n",
    "    print('=========ALM has started=========\\n')\n",
    "    for count in range(0,no_of_chunk):  \n",
    "        # prepare data\n",
    "        n = count + 1\n",
    "        minibatch_data  = preq_data [(n-1)*chunk_size:n*chunk_size]\n",
    "        minibatch_label = preq_label[(n-1)*chunk_size:n*chunk_size]\n",
    "        \n",
    "        # neural network testing      \n",
    "        print('Chunk: {} of {}'.format(n, no_of_chunk))\n",
    "        print('Discriminative Testing: running ...')\n",
    "        parameter.nn.t = n\n",
    "        [parameter.nn] = testing(parameter.nn,minibatch_data,minibatch_label,parameter.ev)\n",
    "        \n",
    "        #metrics calculation\n",
    "        #print(parameter.nn.e)\n",
    "        parameter.Loss[n] = parameter.nn.L[parameter.nn.index]\n",
    "        #parameter.Loss[n] = parameter.nn.e[1]\n",
    "        if(n == 1):\n",
    "            tTest = parameter.nn.sigma.copy()\n",
    "            act = parameter.nn.act.copy()\n",
    "            out = parameter.nn.actualLabel.copy()\n",
    "            parameter.residual_error = np.append(out,parameter.nn.residual_error,axis=0)\n",
    "        else:\n",
    "            tTest = np.append(tTest,parameter.nn.sigma,axis=0)\n",
    "            act = np.append(act,parameter.nn.act,axis=0)\n",
    "            out = np.append(out,parameter.nn.actualLabel,axis=0)\n",
    "            parameter.residual_error = np.append(out,parameter.nn.residual_error,axis=0)\n",
    "            \n",
    "        parameter.cr[n] = parameter.nn.cr;\n",
    "        #classify based on loss\n",
    "        ClassificationRate[n] = np.array(list(parameter.Loss.values())).mean()\n",
    "        #ClassificationRate[n] = parameter.Loss[n]\n",
    "        print('RMSE {}'.format(ClassificationRate[n]))\n",
    "       # print('RMSE {}'.format(parameter.residual_error))\n",
    "        print('Discriminative Testing: ... finished')\n",
    "        \n",
    "        #statistical measure\n",
    "        performance.ev[n] = {}\n",
    "        performance.NumberOfParameters = parameter.nn.mnop\n",
    "        [performance.ev[n]['f_measure'], performance.ev[n]['g_mean'] ,performance.ev[n]['recall'],performance.ev[n]['precision']] = performance_summary(parameter.nn.act, parameter.nn.actualLabel, M)\n",
    "        #last chunk only for testing process\n",
    "        if(n == no_of_chunk):\n",
    "            print('=========ALM is finished=========')\n",
    "            performance.test_time = time.time() - start_time\n",
    "            break\n",
    "            \n",
    "        #layer pruning mechanism\n",
    "        outputCovar = np.zeros((layer,layer,M))\n",
    "        for jj in range(1,layer+1):\n",
    "            for kk in range(1, layer+1):\n",
    "                if parameter.nn.beta[jj] != 0 and parameter.nn.beta[kk] != 0:\n",
    "                    for ll in range(1,M+1):\n",
    "                        temp = np.cov(parameter.nn.a[kk][:,ll],parameter.nn.a[jj][:,ll])\n",
    "#                        print(temp[0][0])\n",
    "                        outputCovar[jj-1,kk-1,ll-1] = temp[0][1]\n",
    "                        covariance[jj-1,kk-1,ll-1] = ((covariance_old[jj-1,kk-1,ll-1]*(n)) + (n/n + 1)*outputCovar[jj-1,kk-1,ll-1] )/(n + 1)\n",
    "\n",
    "        covariance_old = covariance\n",
    "        if layer>1:\n",
    "            merged_list = np.empty([2,2])\n",
    "            for iter1 in range(layer -1):\n",
    "                for hh in range(layer - iter1 -1):\n",
    "                    if parameter.nn.beta[-2] != 0 or parameter.nn.beta[hh] != 0:\n",
    "                        MCI = np.empty([2,2])\n",
    "                        for o in range(M):\n",
    "                            pearson = covariance[-1 - iter1,hh,o]/np.sqrt(covariance[-2,-2,o] * covariance[hh,hh,o])\n",
    "                            MCI[o] = (0.5*(covariance[hh,hh,o] + covariance[-1 - iter1,-1 - iter1,o]) - np.sqrt((covariance[hh,hh,o] + covariance[-1 - iter1,-1 - iter1,o])**2 - 4*covariance[-1 - iter1,-1 - iter1,o]*covariance[hh,hh,o]*(1 - pearson**2)))\n",
    "                        if max(abs(MCI)) < threshold:\n",
    "                            if merged_list.size == 0:\n",
    "                                merged_list[0,0] = layer -1\n",
    "                                merged_list[0,1] = hh\n",
    "                            else:\n",
    "                                nu = np.where(merged_list[:,1:-2]==layer -1)\n",
    "                                nu1 = np.where(merged_list[:,1:-2]== hh -1)\n",
    "                                if np.array(nu).size == 0 and np.array(nu1).size == 0:\n",
    "                                    n_list = np.array([layer-1,hh-1])\n",
    "                                    merged_list = np.array(merged_list,nlist,axis=0)\n",
    "                            break\n",
    "            del_list = []\n",
    "            for itt in range(merged_list.shape[0]):\n",
    "                noOfHighlyCorrelated = np.where(merged_list[itt,:] == 0)\n",
    "                if np.array(noOfHighlyCorrelated).size == 0 :\n",
    "                    if parameter.net.beta[merged_list[itt,1]] > parameter.net.beta[merged_list[itt,2]]:\n",
    "                        deleteLayer = merged_list[itt,2]\n",
    "                    else:\n",
    "                        deleteLayer = merged_list[itt,1]\n",
    "                    del_list.append(deleteLayer)\n",
    "\n",
    "            if len(del_list)>0 and parameter.nn.beta[del_list] != 0:\n",
    "                print(\"'The Hidden Layer no {} is PRUNED around chunk {}]\\n'\".format(del_list, n))\n",
    "                parameter.net.beta[del_list] = 0\n",
    "            parameter.prune_list = parameter.prune_list + len[del_list]\n",
    "            parameter.prune_list_index = np.append(parameter.prune_list_index,del_list)\n",
    "\n",
    "        #Drift detection: output space\n",
    "        if(n>1):\n",
    "            cuttingpoint = 0\n",
    "            pp = minibatch_label.shape[0] #batch size\n",
    "#             F_cut = np.zeros((pp,1))  #accuracy matrix\n",
    "#             F_cut[parameter.nn.wrongClass] = 1\n",
    "            F_cut = list(parameter.Loss.values())\n",
    "            #print(F_cut)\n",
    "            #print(type(F_cut))\n",
    "            Fupper = np.max(F_cut)\n",
    "            Flower = np.min(F_cut)\n",
    "            miu_F = np.mean(F_cut)\n",
    "            \n",
    "            for idx in range(pp):\n",
    "                cut = idx + 1\n",
    "                miu_G = np.mean(F_cut[0:cut])\n",
    "                Gupper = np.max(F_cut[0:cut])\n",
    "                Glower = np.min(F_cut[0:cut])\n",
    "                epsilon_G = (Gupper - Glower) * np.sqrt(((pp)/(2*cut*(pp)) * np.log(1/alpha)))\n",
    "                epsilon_F = (Fupper - Flower) * np.sqrt(((pp)/(2*cut*(pp)) * np.log(1/alpha)))\n",
    "                if ((epsilon_G + miu_G) >= (miu_F + epsilon_F) and cut<pp):\n",
    "                    cuttingpoint = cut\n",
    "                    miu_H = np.mean(F_cut[(cuttingpoint):])\n",
    "                    epsilon_D = (Fupper - Flower) * np.sqrt(((pp-cuttingpoint)/(2*cuttingpoint*(pp-cuttingpoint)) * np.log(1/alpha_d)))\n",
    "                    epsilon_W = (Fupper - Flower) * np.sqrt(((pp-cuttingpoint)/(2*cuttingpoint*(pp-cuttingpoint)) * np.log(1/alpha_w)))\n",
    "                    break\n",
    "            if(cuttingpoint == 0):\n",
    "                miu_H = miu_F\n",
    "                epsilon_D = (Fupper - Flower) * np.sqrt(((pp)/(2*cut*(pp)) * np.log(1/alpha_d)))\n",
    "                epsilon_W = (Fupper - Flower) * np.sqrt(((pp)/(2*cut*(pp)) * np.log(1/alpha_w)))\n",
    "            \n",
    "            #DRIFT STATUS\n",
    "            if((np.abs(miu_G - miu_H)) > epsilon_D and cuttingpoint>1):\n",
    "                st = 1\n",
    "                print('Drift state: DRIFT')\n",
    "                layer = layer+1\n",
    "                parameter.nn.n = parameter.nn.n + 1\n",
    "                parameter.nn.hiddenLayer = layer\n",
    "                print('The new Layer no {} is FORMED around chunk {}'.format(layer, n))\n",
    "                \n",
    "                #Initiate NN weight parameters\n",
    "                ii = parameter.nn.W[layer-1].shape[0]\n",
    "                parameter.nn.W[layer] = np.random.normal(0,np.sqrt(2/(ii+1)),size = (1, (ii+1)))    \n",
    "                parameter.nn.vW[layer] = np.zeros((1,ii+1))\n",
    "                parameter.nn.dW[layer] = np.zeros((1,ii+1))\n",
    "                \n",
    "                #Initiate new classifier weight\n",
    "                parameter.nn.Ws[layer]  = np.random.normal(0,1,size = (M,2))    \n",
    "                parameter.nn.vWs[layer] = np.zeros((M,2))\n",
    "                parameter.nn.dWs[layer] = np.zeros((M,2))\n",
    "                \n",
    "                #Initiate new voting weight\n",
    "                parameter.nn.beta[layer] = 1\n",
    "                parameter.nn.betaOld[layer] = 1\n",
    "                parameter.nn.p[layer] = 1\n",
    "                \n",
    "                # Initiate iterative parameters\n",
    "                parameter.ev[layer] = {}\n",
    "                parameter.ev[layer]['layer ']      = layer\n",
    "                parameter.ev[layer]['kl']          = 0\n",
    "                parameter.ev[layer]['K']           = 1\n",
    "                parameter.ev[layer]['cr']           = 0\n",
    "                parameter.ev[layer]['node']        = {}\n",
    "                parameter.ev[layer]['miu_NS_old']  = 0\n",
    "                parameter.ev[layer]['var_NS_old']  = 0\n",
    "                parameter.ev[layer]['miu_NHS_old'] = 0\n",
    "                parameter.ev[layer]['var_NHS_old'] = 0\n",
    "                parameter.ev[layer]['miumin_NS']   = []\n",
    "                parameter.ev[layer]['miumin_NHS']  = []\n",
    "                parameter.ev[layer]['stdmin_NS']   = []\n",
    "                parameter.ev[layer]['stdmin_NHS']  = []\n",
    "                parameter.ev[layer]['BIAS2']       = {}\n",
    "                parameter.ev[layer]['VAR']         = {} \n",
    "                #initiate covariance\n",
    "                covariance = np.zeros((5,5,M))\n",
    "                covariance_old  = covariance\n",
    "                \n",
    "                #check buffer\n",
    "                if(len(buffer_x) == 0):\n",
    "                    pass\n",
    "#                     h = parameter.nn.a[len(parameter.nn.a)][:,1:]\n",
    "#                     z = minibatch_label\n",
    "                else:\n",
    "                    h = np.append(buffer_x[:,1:],parameter.nn.a[len(parameter.nn.a)][:,1:],axis=0)\n",
    "                    if(len(buffer_T) == 0):\n",
    "                        z = np.append(buffer_T,minibatch_label ,axis=0)\n",
    "                    else:\n",
    "                        z = minibatch_label\n",
    "                        \n",
    "                #Discriminative training for new layer\n",
    "                    print('Discriminative Training for new layer: running ...')\n",
    "                    parameter = netfeedforward(parameter,h,z)\n",
    "                    print('Discriminative Training for new layer: ... finished')\n",
    "                #clear buffer\n",
    "                buffer_x = []\n",
    "                buffer_T = []\n",
    "            elif((np.abs(miu_G - miu_H)) >= epsilon_W and (np.abs(miu_G - miu_H)) < epsilon_D):\n",
    "                st = 2\n",
    "                print('Drift state: WARNING')\n",
    "                buffer_x = minibatch_data\n",
    "                buffer_T = minibatch_label\n",
    "            else:\n",
    "                st = 3\n",
    "                print('Drift state: STABLE')\n",
    "                if(len(buffer_x) == 0):\n",
    "                    pass\n",
    "                else:\n",
    "#                    buffer_x = netffhl(parameter.nn, buffer_x)\n",
    "                    h = np.append(buffer_x[:,1:],parameter.nn.a[len(parameter.nn.a)][:,1:],axis=0)\n",
    "                    if(len(buffer_T) == 0):\n",
    "                        z = np.append(buffer_T,minibatch_label ,axis=0)\n",
    "                    else:\n",
    "                        z = minibatch_label\n",
    "                    print('Discriminative Training for new layer: running ...')\n",
    "                    parameter = netfeedforward(parameter,h,z)\n",
    "                    print('Discriminative Training for new layer: ... finished')\n",
    "                buffer_x = []\n",
    "                buffer_T = []\n",
    "        else:\n",
    "            st = 3\n",
    "            print('Drift state: STABLE')\n",
    "            buffer_x = []\n",
    "            buffer_T = []\n",
    "        drift[n] = st\n",
    "        HL[n] = checkbeta(parameter.nn.beta)\n",
    "        parameter.wl[n] = parameter.nn.index\n",
    "        \n",
    "        #Discriminative training for winning layer\n",
    "        if(st != 1):\n",
    "            print('Discriminative Training: running ...')\n",
    "            parameter = training(parameter, minibatch_label)\n",
    "            print('Discriminative Training: ... finished')\n",
    "            \n",
    "        #Clear current data chunk\n",
    "        parameter.nn.a = {}\n",
    "        print('=========Hidden layer number {} was updated========='.format(parameter.nn.index))\n",
    "    return parameter,performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========Autonomous Deep Learning has started=========\n",
      "\n",
      "Chunk: 1 of 82\n",
      "Discriminative Testing: running ...\n",
      "RMSE 0.2743646675635627\n",
      "Discriminative Testing: ... finished\n",
      "Drift state: STABLE\n",
      "Discriminative Training: running ...\n",
      "The new node no 2 is FORMED around sample 4\n",
      "The new node no 3 is FORMED around sample 10\n",
      "The node no 3 is PRUNED around sample 28\n",
      "The node no 2 is PRUNED around sample 29\n",
      "Discriminative Training: ... finished\n",
      "=========Hidden layer number 1 was updated=========\n",
      "Chunk: 2 of 82\n",
      "Discriminative Testing: running ...\n",
      "RMSE 0.17540863466268503\n",
      "Discriminative Testing: ... finished\n",
      "Drift state: STABLE\n",
      "Discriminative Training: running ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Bryan\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "C:\\Users\\Bryan\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:85: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discriminative Training: ... finished\n",
      "=========Hidden layer number 1 was updated=========\n",
      "Chunk: 3 of 82\n",
      "Discriminative Testing: running ...\n",
      "RMSE 0.12497888573823872\n",
      "Discriminative Testing: ... finished\n",
      "Drift state: STABLE\n",
      "Discriminative Training: running ...\n",
      "Discriminative Training: ... finished\n",
      "=========Hidden layer number 1 was updated=========\n",
      "Chunk: 4 of 82\n",
      "Discriminative Testing: running ...\n",
      "RMSE 0.10416223063723573\n",
      "Discriminative Testing: ... finished\n",
      "Drift state: STABLE\n",
      "Discriminative Training: running ...\n",
      "The new node no 2 is FORMED around sample 9\n",
      "The node no 2 is PRUNED around sample 10\n",
      "The new node no 2 is FORMED around sample 451\n",
      "The node no 2 is PRUNED around sample 452\n",
      "Discriminative Training: ... finished\n",
      "=========Hidden layer number 1 was updated=========\n",
      "Chunk: 5 of 82\n",
      "Discriminative Testing: running ...\n",
      "RMSE 0.09361637919717428\n",
      "Discriminative Testing: ... finished\n",
      "Drift state: STABLE\n",
      "Discriminative Training: running ...\n",
      "Discriminative Training: ... finished\n",
      "=========Hidden layer number 1 was updated=========\n",
      "Chunk: 6 of 82\n",
      "Discriminative Testing: running ...\n",
      "RMSE 0.09338165162351125\n",
      "Discriminative Testing: ... finished\n",
      "Drift state: STABLE\n",
      "Discriminative Training: running ...\n",
      "Discriminative Training: ... finished\n",
      "=========Hidden layer number 1 was updated=========\n",
      "Chunk: 7 of 82\n",
      "Discriminative Testing: running ...\n",
      "RMSE 0.0882955653788799\n",
      "Discriminative Testing: ... finished\n",
      "Drift state: STABLE\n",
      "Discriminative Training: running ...\n",
      "Discriminative Training: ... finished\n",
      "=========Hidden layer number 1 was updated=========\n",
      "Chunk: 8 of 82\n",
      "Discriminative Testing: running ...\n",
      "RMSE 0.08430889433388605\n",
      "Discriminative Testing: ... finished\n",
      "Drift state: STABLE\n",
      "Discriminative Training: running ...\n",
      "Discriminative Training: ... finished\n",
      "=========Hidden layer number 1 was updated=========\n",
      "Chunk: 9 of 82\n",
      "Discriminative Testing: running ...\n",
      "RMSE 0.07687108717315899\n",
      "Discriminative Testing: ... finished\n",
      "Drift state: STABLE\n",
      "Discriminative Training: running ...\n",
      "Discriminative Training: ... finished\n",
      "=========Hidden layer number 1 was updated=========\n",
      "Chunk: 10 of 82\n",
      "Discriminative Testing: running ...\n",
      "RMSE 0.07010013420388075\n",
      "Discriminative Testing: ... finished\n",
      "Drift state: STABLE\n",
      "Discriminative Training: running ...\n",
      "Discriminative Training: ... finished\n",
      "=========Hidden layer number 1 was updated=========\n",
      "Chunk: 11 of 82\n",
      "Discriminative Testing: running ...\n",
      "RMSE 0.06705380575992824\n",
      "Discriminative Testing: ... finished\n",
      "Drift state: STABLE\n",
      "Discriminative Training: running ...\n",
      "Discriminative Training: ... finished\n",
      "=========Hidden layer number 1 was updated=========\n",
      "Chunk: 12 of 82\n",
      "Discriminative Testing: running ...\n",
      "RMSE 0.06499730381700426\n",
      "Discriminative Testing: ... finished\n",
      "Drift state: STABLE\n",
      "Discriminative Training: running ...\n",
      "Discriminative Training: ... finished\n",
      "=========Hidden layer number 1 was updated=========\n",
      "Chunk: 13 of 82\n",
      "Discriminative Testing: running ...\n",
      "RMSE 0.061847846708937164\n",
      "Discriminative Testing: ... finished\n",
      "Drift state: STABLE\n",
      "Discriminative Training: running ...\n",
      "Discriminative Training: ... finished\n",
      "=========Hidden layer number 1 was updated=========\n",
      "Chunk: 14 of 82\n",
      "Discriminative Testing: running ...\n",
      "RMSE 0.05854165212640693\n",
      "Discriminative Testing: ... finished\n",
      "Drift state: STABLE\n",
      "Discriminative Training: running ...\n",
      "Discriminative Training: ... finished\n",
      "=========Hidden layer number 1 was updated=========\n",
      "Chunk: 15 of 82\n",
      "Discriminative Testing: running ...\n",
      "RMSE 0.059034702863591706\n",
      "Discriminative Testing: ... finished\n",
      "Drift state: STABLE\n",
      "Discriminative Training: running ...\n",
      "Discriminative Training: ... finished\n",
      "=========Hidden layer number 1 was updated=========\n",
      "Chunk: 16 of 82\n",
      "Discriminative Testing: running ...\n",
      "RMSE 0.056008595220722324\n",
      "Discriminative Testing: ... finished\n",
      "Drift state: STABLE\n",
      "Discriminative Training: running ...\n",
      "Discriminative Training: ... finished\n",
      "=========Hidden layer number 1 was updated=========\n",
      "Chunk: 17 of 82\n",
      "Discriminative Testing: running ...\n",
      "RMSE 0.05393763263354075\n",
      "Discriminative Testing: ... finished\n",
      "Drift state: STABLE\n",
      "Discriminative Training: running ...\n",
      "Discriminative Training: ... finished\n",
      "=========Hidden layer number 1 was updated=========\n",
      "Chunk: 18 of 82\n",
      "Discriminative Testing: running ...\n",
      "RMSE 0.05154441870181467\n",
      "Discriminative Testing: ... finished\n",
      "Drift state: STABLE\n",
      "Discriminative Training: running ...\n",
      "Discriminative Training: ... finished\n",
      "=========Hidden layer number 1 was updated=========\n",
      "Chunk: 19 of 82\n",
      "Discriminative Testing: running ...\n",
      "RMSE 0.04909887244865391\n",
      "Discriminative Testing: ... finished\n",
      "Drift state: STABLE\n",
      "Discriminative Training: running ...\n",
      "Discriminative Training: ... finished\n",
      "=========Hidden layer number 1 was updated=========\n",
      "Chunk: 20 of 82\n",
      "Discriminative Testing: running ...\n",
      "RMSE 0.052244645900305996\n",
      "Discriminative Testing: ... finished\n",
      "Drift state: STABLE\n",
      "Discriminative Training: running ...\n",
      "Discriminative Training: ... finished\n",
      "=========Hidden layer number 1 was updated=========\n",
      "Chunk: 21 of 82\n",
      "Discriminative Testing: running ...\n",
      "RMSE 0.052431498703820474\n",
      "Discriminative Testing: ... finished\n",
      "Drift state: STABLE\n",
      "Discriminative Training: running ...\n",
      "Discriminative Training: ... finished\n",
      "=========Hidden layer number 1 was updated=========\n",
      "Chunk: 22 of 82\n",
      "Discriminative Testing: running ...\n",
      "RMSE 0.05164924854081987\n",
      "Discriminative Testing: ... finished\n",
      "Drift state: STABLE\n",
      "Discriminative Training: running ...\n",
      "Discriminative Training: ... finished\n",
      "=========Hidden layer number 1 was updated=========\n",
      "Chunk: 23 of 82\n",
      "Discriminative Testing: running ...\n",
      "RMSE 0.05154710469193567\n",
      "Discriminative Testing: ... finished\n",
      "Drift state: STABLE\n",
      "Discriminative Training: running ...\n",
      "Discriminative Training: ... finished\n",
      "=========Hidden layer number 1 was updated=========\n",
      "Chunk: 24 of 82\n",
      "Discriminative Testing: running ...\n",
      "RMSE 0.05126091027664612\n",
      "Discriminative Testing: ... finished\n",
      "Drift state: STABLE\n",
      "Discriminative Training: running ...\n",
      "Discriminative Training: ... finished\n",
      "=========Hidden layer number 1 was updated=========\n",
      "Chunk: 25 of 82\n",
      "Discriminative Testing: running ...\n",
      "RMSE 0.04983349848515543\n",
      "Discriminative Testing: ... finished\n",
      "Drift state: STABLE\n",
      "Discriminative Training: running ...\n",
      "Discriminative Training: ... finished\n",
      "=========Hidden layer number 1 was updated=========\n",
      "Chunk: 26 of 82\n",
      "Discriminative Testing: running ...\n",
      "RMSE 0.04877266627145453\n",
      "Discriminative Testing: ... finished\n",
      "Drift state: STABLE\n",
      "Discriminative Training: running ...\n",
      "Discriminative Training: ... finished\n",
      "=========Hidden layer number 1 was updated=========\n",
      "Chunk: 27 of 82\n",
      "Discriminative Testing: running ...\n",
      "RMSE 0.05037885033405392\n",
      "Discriminative Testing: ... finished\n",
      "Drift state: STABLE\n",
      "Discriminative Training: running ...\n",
      "Discriminative Training: ... finished\n",
      "=========Hidden layer number 1 was updated=========\n",
      "Chunk: 28 of 82\n",
      "Discriminative Testing: running ...\n",
      "RMSE 0.04947283371695284\n",
      "Discriminative Testing: ... finished\n",
      "Drift state: STABLE\n",
      "Discriminative Training: running ...\n",
      "Discriminative Training: ... finished\n",
      "=========Hidden layer number 1 was updated=========\n",
      "Chunk: 29 of 82\n",
      "Discriminative Testing: running ...\n",
      "RMSE 0.04825957549550903\n",
      "Discriminative Testing: ... finished\n",
      "Drift state: STABLE\n",
      "Discriminative Training: running ...\n",
      "The new node no 2 is FORMED around sample 11\n",
      "The node no 1 is PRUNED around sample 12\n",
      "Discriminative Training: ... finished\n",
      "=========Hidden layer number 1 was updated=========\n",
      "Chunk: 30 of 82\n",
      "Discriminative Testing: running ...\n",
      "RMSE 0.04973001573140379\n",
      "Discriminative Testing: ... finished\n",
      "Drift state: STABLE\n",
      "Discriminative Training: running ...\n",
      "Discriminative Training: ... finished\n",
      "=========Hidden layer number 1 was updated=========\n",
      "Chunk: 31 of 82\n",
      "Discriminative Testing: running ...\n",
      "RMSE 0.048332385768143485\n",
      "Discriminative Testing: ... finished\n",
      "Drift state: STABLE\n",
      "Discriminative Training: running ...\n",
      "Discriminative Training: ... finished\n",
      "=========Hidden layer number 1 was updated=========\n",
      "Chunk: 32 of 82\n",
      "Discriminative Testing: running ...\n",
      "RMSE 0.046929554331338746\n",
      "Discriminative Testing: ... finished\n",
      "Drift state: STABLE\n",
      "Discriminative Training: running ...\n",
      "Discriminative Training: ... finished\n",
      "=========Hidden layer number 1 was updated=========\n",
      "Chunk: 33 of 82\n",
      "Discriminative Testing: running ...\n",
      "RMSE 0.04637346821381061\n",
      "Discriminative Testing: ... finished\n",
      "Drift state: STABLE\n",
      "Discriminative Training: running ...\n",
      "Discriminative Training: ... finished\n",
      "=========Hidden layer number 1 was updated=========\n",
      "Chunk: 34 of 82\n",
      "Discriminative Testing: running ...\n",
      "RMSE 0.04637556803121241\n",
      "Discriminative Testing: ... finished\n",
      "Drift state: STABLE\n",
      "Discriminative Training: running ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discriminative Training: ... finished\n",
      "=========Hidden layer number 1 was updated=========\n",
      "Chunk: 35 of 82\n",
      "Discriminative Testing: running ...\n",
      "RMSE 0.048655745952607195\n",
      "Discriminative Testing: ... finished\n",
      "Drift state: STABLE\n",
      "Discriminative Training: running ...\n",
      "Discriminative Training: ... finished\n",
      "=========Hidden layer number 1 was updated=========\n",
      "Chunk: 36 of 82\n",
      "Discriminative Testing: running ...\n",
      "RMSE 0.04731286292410213\n",
      "Discriminative Testing: ... finished\n",
      "Drift state: STABLE\n",
      "Discriminative Training: running ...\n",
      "Discriminative Training: ... finished\n",
      "=========Hidden layer number 1 was updated=========\n",
      "Chunk: 37 of 82\n",
      "Discriminative Testing: running ...\n",
      "RMSE 0.047559628254318496\n",
      "Discriminative Testing: ... finished\n",
      "Drift state: STABLE\n",
      "Discriminative Training: running ...\n",
      "Discriminative Training: ... finished\n",
      "=========Hidden layer number 1 was updated=========\n",
      "Chunk: 38 of 82\n",
      "Discriminative Testing: running ...\n",
      "RMSE 0.046954279034670546\n",
      "Discriminative Testing: ... finished\n",
      "Drift state: STABLE\n",
      "Discriminative Training: running ...\n",
      "Discriminative Training: ... finished\n",
      "=========Hidden layer number 1 was updated=========\n",
      "Chunk: 39 of 82\n",
      "Discriminative Testing: running ...\n",
      "RMSE 0.04645822695524466\n",
      "Discriminative Testing: ... finished\n",
      "Drift state: STABLE\n",
      "Discriminative Training: running ...\n",
      "Discriminative Training: ... finished\n",
      "=========Hidden layer number 1 was updated=========\n",
      "Chunk: 40 of 82\n",
      "Discriminative Testing: running ...\n",
      "RMSE 0.046313949228322895\n",
      "Discriminative Testing: ... finished\n",
      "Drift state: STABLE\n",
      "Discriminative Training: running ...\n",
      "Discriminative Training: ... finished\n",
      "=========Hidden layer number 1 was updated=========\n",
      "Chunk: 41 of 82\n",
      "Discriminative Testing: running ...\n",
      "RMSE 0.04605442770866172\n",
      "Discriminative Testing: ... finished\n",
      "Drift state: STABLE\n",
      "Discriminative Training: running ...\n",
      "Discriminative Training: ... finished\n",
      "=========Hidden layer number 1 was updated=========\n",
      "Chunk: 42 of 82\n",
      "Discriminative Testing: running ...\n",
      "RMSE 0.04675203171054175\n",
      "Discriminative Testing: ... finished\n",
      "Drift state: STABLE\n",
      "Discriminative Training: running ...\n",
      "Discriminative Training: ... finished\n",
      "=========Hidden layer number 1 was updated=========\n",
      "Chunk: 43 of 82\n",
      "Discriminative Testing: running ...\n",
      "RMSE 0.047076597166918616\n",
      "Discriminative Testing: ... finished\n",
      "Drift state: STABLE\n",
      "Discriminative Training: running ...\n",
      "Discriminative Training: ... finished\n",
      "=========Hidden layer number 1 was updated=========\n",
      "Chunk: 44 of 82\n",
      "Discriminative Testing: running ...\n",
      "RMSE 0.047314938857785954\n",
      "Discriminative Testing: ... finished\n",
      "Drift state: STABLE\n",
      "Discriminative Training: running ...\n",
      "Discriminative Training: ... finished\n",
      "=========Hidden layer number 1 was updated=========\n",
      "Chunk: 45 of 82\n",
      "Discriminative Testing: running ...\n",
      "RMSE 0.0485526942506506\n",
      "Discriminative Testing: ... finished\n",
      "Drift state: STABLE\n",
      "Discriminative Training: running ...\n",
      "Discriminative Training: ... finished\n",
      "=========Hidden layer number 1 was updated=========\n",
      "Chunk: 46 of 82\n",
      "Discriminative Testing: running ...\n",
      "RMSE 0.04769617951542725\n",
      "Discriminative Testing: ... finished\n",
      "Drift state: STABLE\n",
      "Discriminative Training: running ...\n",
      "Discriminative Training: ... finished\n",
      "=========Hidden layer number 1 was updated=========\n",
      "Chunk: 47 of 82\n",
      "Discriminative Testing: running ...\n",
      "RMSE 0.04800739878218501\n",
      "Discriminative Testing: ... finished\n",
      "Drift state: STABLE\n",
      "Discriminative Training: running ...\n",
      "Discriminative Training: ... finished\n",
      "=========Hidden layer number 1 was updated=========\n",
      "Chunk: 48 of 82\n",
      "Discriminative Testing: running ...\n",
      "RMSE 0.04817916693107263\n",
      "Discriminative Testing: ... finished\n",
      "Drift state: STABLE\n",
      "Discriminative Training: running ...\n",
      "Discriminative Training: ... finished\n",
      "=========Hidden layer number 1 was updated=========\n",
      "Chunk: 49 of 82\n",
      "Discriminative Testing: running ...\n",
      "RMSE 0.050855287357522164\n",
      "Discriminative Testing: ... finished\n",
      "Drift state: STABLE\n",
      "Discriminative Training: running ...\n",
      "Discriminative Training: ... finished\n",
      "=========Hidden layer number 1 was updated=========\n",
      "Chunk: 50 of 82\n",
      "Discriminative Testing: running ...\n",
      "RMSE 0.050612803030355605\n",
      "Discriminative Testing: ... finished\n",
      "Drift state: STABLE\n",
      "Discriminative Training: running ...\n",
      "Discriminative Training: ... finished\n",
      "=========Hidden layer number 1 was updated=========\n",
      "Chunk: 51 of 82\n",
      "Discriminative Testing: running ...\n",
      "RMSE 0.05019218848667159\n",
      "Discriminative Testing: ... finished\n",
      "Drift state: STABLE\n",
      "Discriminative Training: running ...\n",
      "Discriminative Training: ... finished\n",
      "=========Hidden layer number 1 was updated=========\n",
      "Chunk: 52 of 82\n",
      "Discriminative Testing: running ...\n",
      "RMSE 0.049510869216715804\n",
      "Discriminative Testing: ... finished\n",
      "Drift state: STABLE\n",
      "Discriminative Training: running ...\n",
      "Discriminative Training: ... finished\n",
      "=========Hidden layer number 1 was updated=========\n",
      "Chunk: 53 of 82\n",
      "Discriminative Testing: running ...\n",
      "RMSE 0.04911172911904409\n",
      "Discriminative Testing: ... finished\n",
      "Drift state: STABLE\n",
      "Discriminative Training: running ...\n",
      "Discriminative Training: ... finished\n",
      "=========Hidden layer number 1 was updated=========\n",
      "Chunk: 54 of 82\n",
      "Discriminative Testing: running ...\n",
      "RMSE 0.04940525659317369\n",
      "Discriminative Testing: ... finished\n",
      "Drift state: STABLE\n",
      "Discriminative Training: running ...\n",
      "Discriminative Training: ... finished\n",
      "=========Hidden layer number 1 was updated=========\n",
      "Chunk: 55 of 82\n",
      "Discriminative Testing: running ...\n",
      "RMSE 0.04978542989707331\n",
      "Discriminative Testing: ... finished\n",
      "Drift state: STABLE\n",
      "Discriminative Training: running ...\n",
      "Discriminative Training: ... finished\n",
      "=========Hidden layer number 1 was updated=========\n",
      "Chunk: 56 of 82\n",
      "Discriminative Testing: running ...\n",
      "RMSE 0.04992091352603188\n",
      "Discriminative Testing: ... finished\n",
      "Drift state: STABLE\n",
      "Discriminative Training: running ...\n",
      "Discriminative Training: ... finished\n",
      "=========Hidden layer number 1 was updated=========\n",
      "Chunk: 57 of 82\n",
      "Discriminative Testing: running ...\n",
      "RMSE 0.04933767943013274\n",
      "Discriminative Testing: ... finished\n",
      "Drift state: STABLE\n",
      "Discriminative Training: running ...\n",
      "Discriminative Training: ... finished\n",
      "=========Hidden layer number 1 was updated=========\n",
      "Chunk: 58 of 82\n",
      "Discriminative Testing: running ...\n",
      "RMSE 0.04880096557472592\n",
      "Discriminative Testing: ... finished\n",
      "Drift state: STABLE\n",
      "Discriminative Training: running ...\n",
      "Discriminative Training: ... finished\n",
      "=========Hidden layer number 1 was updated=========\n",
      "Chunk: 59 of 82\n",
      "Discriminative Testing: running ...\n",
      "RMSE 0.05070283578763328\n",
      "Discriminative Testing: ... finished\n",
      "Drift state: STABLE\n",
      "Discriminative Training: running ...\n",
      "Discriminative Training: ... finished\n",
      "=========Hidden layer number 1 was updated=========\n",
      "Chunk: 60 of 82\n",
      "Discriminative Testing: running ...\n",
      "RMSE 0.05062132314229854\n",
      "Discriminative Testing: ... finished\n",
      "Drift state: STABLE\n",
      "Discriminative Training: running ...\n",
      "Discriminative Training: ... finished\n",
      "=========Hidden layer number 1 was updated=========\n",
      "Chunk: 61 of 82\n",
      "Discriminative Testing: running ...\n",
      "RMSE 0.05076800157521083\n",
      "Discriminative Testing: ... finished\n",
      "Drift state: STABLE\n",
      "Discriminative Training: running ...\n",
      "Discriminative Training: ... finished\n",
      "=========Hidden layer number 1 was updated=========\n",
      "Chunk: 62 of 82\n",
      "Discriminative Testing: running ...\n",
      "RMSE 0.05005135339498276\n",
      "Discriminative Testing: ... finished\n",
      "Drift state: STABLE\n",
      "Discriminative Training: running ...\n",
      "Discriminative Training: ... finished\n",
      "=========Hidden layer number 1 was updated=========\n",
      "Chunk: 63 of 82\n",
      "Discriminative Testing: running ...\n",
      "RMSE 0.04950199011981832\n",
      "Discriminative Testing: ... finished\n",
      "Drift state: STABLE\n",
      "Discriminative Training: running ...\n",
      "Discriminative Training: ... finished\n",
      "=========Hidden layer number 1 was updated=========\n",
      "Chunk: 64 of 82\n",
      "Discriminative Testing: running ...\n",
      "RMSE 0.0495585424305405\n",
      "Discriminative Testing: ... finished\n",
      "Drift state: STABLE\n",
      "Discriminative Training: running ...\n",
      "Discriminative Training: ... finished\n",
      "=========Hidden layer number 1 was updated=========\n",
      "Chunk: 65 of 82\n",
      "Discriminative Testing: running ...\n",
      "RMSE 0.04924099410232217\n",
      "Discriminative Testing: ... finished\n",
      "Drift state: STABLE\n",
      "Discriminative Training: running ...\n",
      "Discriminative Training: ... finished\n",
      "=========Hidden layer number 1 was updated=========\n",
      "Chunk: 66 of 82\n",
      "Discriminative Testing: running ...\n",
      "RMSE 0.04917657136373759\n",
      "Discriminative Testing: ... finished\n",
      "Drift state: STABLE\n",
      "Discriminative Training: running ...\n",
      "Discriminative Training: ... finished\n",
      "=========Hidden layer number 1 was updated=========\n",
      "Chunk: 67 of 82\n",
      "Discriminative Testing: running ...\n",
      "RMSE 0.05033181132565027\n",
      "Discriminative Testing: ... finished\n",
      "Drift state: STABLE\n",
      "Discriminative Training: running ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discriminative Training: ... finished\n",
      "=========Hidden layer number 1 was updated=========\n",
      "Chunk: 68 of 82\n",
      "Discriminative Testing: running ...\n",
      "RMSE 0.049782869004861466\n",
      "Discriminative Testing: ... finished\n",
      "Drift state: STABLE\n",
      "Discriminative Training: running ...\n",
      "Discriminative Training: ... finished\n",
      "=========Hidden layer number 1 was updated=========\n",
      "Chunk: 69 of 82\n",
      "Discriminative Testing: running ...\n",
      "RMSE 0.0500431833300051\n",
      "Discriminative Testing: ... finished\n",
      "Drift state: STABLE\n",
      "Discriminative Training: running ...\n",
      "Discriminative Training: ... finished\n",
      "=========Hidden layer number 1 was updated=========\n",
      "Chunk: 70 of 82\n",
      "Discriminative Testing: running ...\n",
      "RMSE 0.04935601765262762\n",
      "Discriminative Testing: ... finished\n",
      "Drift state: STABLE\n",
      "Discriminative Training: running ...\n",
      "Discriminative Training: ... finished\n",
      "=========Hidden layer number 1 was updated=========\n",
      "Chunk: 71 of 82\n",
      "Discriminative Testing: running ...\n",
      "RMSE 0.04923329045181425\n",
      "Discriminative Testing: ... finished\n",
      "Drift state: STABLE\n",
      "Discriminative Training: running ...\n",
      "Discriminative Training: ... finished\n",
      "=========Hidden layer number 1 was updated=========\n",
      "Chunk: 72 of 82\n",
      "Discriminative Testing: running ...\n",
      "RMSE 0.048778789594820404\n",
      "Discriminative Testing: ... finished\n",
      "Drift state: STABLE\n",
      "Discriminative Training: running ...\n",
      "Discriminative Training: ... finished\n",
      "=========Hidden layer number 1 was updated=========\n",
      "Chunk: 73 of 82\n",
      "Discriminative Testing: running ...\n",
      "RMSE 0.04853853179711355\n",
      "Discriminative Testing: ... finished\n",
      "Drift state: STABLE\n",
      "Discriminative Training: running ...\n",
      "Discriminative Training: ... finished\n",
      "=========Hidden layer number 1 was updated=========\n",
      "Chunk: 74 of 82\n",
      "Discriminative Testing: running ...\n",
      "RMSE 0.048829806811234173\n",
      "Discriminative Testing: ... finished\n",
      "Drift state: STABLE\n",
      "Discriminative Training: running ...\n",
      "Discriminative Training: ... finished\n",
      "=========Hidden layer number 1 was updated=========\n",
      "Chunk: 75 of 82\n",
      "Discriminative Testing: running ...\n",
      "RMSE 0.04861862891347223\n",
      "Discriminative Testing: ... finished\n",
      "Drift state: STABLE\n",
      "Discriminative Training: running ...\n",
      "Discriminative Training: ... finished\n",
      "=========Hidden layer number 1 was updated=========\n",
      "Chunk: 76 of 82\n",
      "Discriminative Testing: running ...\n",
      "RMSE 0.04923847816624873\n",
      "Discriminative Testing: ... finished\n",
      "Drift state: STABLE\n",
      "Discriminative Training: running ...\n",
      "Discriminative Training: ... finished\n",
      "=========Hidden layer number 1 was updated=========\n",
      "Chunk: 77 of 82\n",
      "Discriminative Testing: running ...\n",
      "RMSE 0.04921328493130522\n",
      "Discriminative Testing: ... finished\n",
      "Drift state: STABLE\n",
      "Discriminative Training: running ...\n",
      "Discriminative Training: ... finished\n",
      "=========Hidden layer number 1 was updated=========\n",
      "Chunk: 78 of 82\n",
      "Discriminative Testing: running ...\n",
      "RMSE 0.05043846594410844\n",
      "Discriminative Testing: ... finished\n",
      "Drift state: STABLE\n",
      "Discriminative Training: running ...\n",
      "Discriminative Training: ... finished\n",
      "=========Hidden layer number 1 was updated=========\n",
      "Chunk: 79 of 82\n",
      "Discriminative Testing: running ...\n",
      "RMSE 0.05179043751087806\n",
      "Discriminative Testing: ... finished\n",
      "Drift state: STABLE\n",
      "Discriminative Training: running ...\n",
      "Discriminative Training: ... finished\n",
      "=========Hidden layer number 1 was updated=========\n",
      "Chunk: 80 of 82\n",
      "Discriminative Testing: running ...\n",
      "RMSE 0.05135058183065751\n",
      "Discriminative Testing: ... finished\n",
      "Drift state: STABLE\n",
      "Discriminative Training: running ...\n",
      "Discriminative Training: ... finished\n",
      "=========Hidden layer number 1 was updated=========\n",
      "Chunk: 81 of 82\n",
      "Discriminative Testing: running ...\n",
      "RMSE 0.05160125209350618\n",
      "Discriminative Testing: ... finished\n",
      "Drift state: STABLE\n",
      "Discriminative Training: running ...\n",
      "Discriminative Training: ... finished\n",
      "=========Hidden layer number 1 was updated=========\n",
      "Chunk: 82 of 82\n",
      "Discriminative Testing: running ...\n",
      "RMSE 0.051521617746189366\n",
      "Discriminative Testing: ... finished\n",
      "=========Parallel Autonomous Deep Learning is finished=========\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    parameter, performance = test_ADL(data,n_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[29.0, 0.0]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameter.nn.mnop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "#number of nodes\n",
    "print(parameter.ev[1]['K'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.590510129928589\n"
     ]
    }
   ],
   "source": [
    "print(performance.test_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 0.2743646675635627, 2: 0.07645260176180736, 3: 0.024119387889346106, 4: 0.04171226533422676, 5: 0.051432973436928424, 6: 0.09220801375519615, 7: 0.05777904791109174, 8: 0.05640219701892927, 9: 0.017368629887342606, 10: 0.009161557480376623, 11: 0.03659052132040301, 12: 0.04237578244484069, 13: 0.024054361412131933, 14: 0.015561122553513964, 15: 0.06593741318417858, 16: 0.010616980577681262, 17: 0.020802231238635574, 18: 0.010859781862471416, 19: 0.005079039891760134, 20: 0.11201434148169576, 21: 0.056168554774109876, 22: 0.03522199511780711, 23: 0.04929994001648327, 24: 0.04467843872498682, 25: 0.015575615489378634, 26: 0.02225186092893211, 27: 0.0921396359616379, 28: 0.025010385055223683, 29: 0.014288345295082376, 30: 0.09237278257235164, 31: 0.0064034868703345274, 32: 0.003441779790391901, 33: 0.02857871245291011, 34: 0.046444862005471786, 35: 0.1261817952800299, 36: 0.00031195692642501895, 37: 0.056443180142107634, 38: 0.02455635790769612, 39: 0.02760824793706115, 40: 0.04068711787837429, 41: 0.03567356692221475, 42: 0.075353795787623, 43: 0.06070834633474685, 44: 0.0575636315650815, 45: 0.1030139315366946, 46: 0.009153016430376464, 47: 0.062323485053042346, 48: 0.05625226992879082, 49: 0.17930906782710007, 50: 0.03873107099919394, 51: 0.029161461302471037, 52: 0.014763586448970422, 53: 0.028356444040114948, 54: 0.06496221272204243, 55: 0.07031478830765257, 56: 0.05737251311875341, 57: 0.016676570059780814, 58: 0.018208275816537743, 59: 0.16101130813625975, 60: 0.04581207706754894, 61: 0.059568707549948356, 62: 0.006335814401070361, 63: 0.015441467059622788, 64: 0.05312133800603864, 65: 0.028917901096348494, 66: 0.044989093355740035, 67: 0.12657764881188693, 68: 0.013003733512011707, 69: 0.06774455743977172, 70: 0.0019415859135817567, 71: 0.040642386394878535, 72: 0.01650922874825754, 73: 0.03123997036222025, 74: 0.07009288284203954, 75: 0.03299146447908859, 76: 0.09572717212448645, 77: 0.047298599075598344, 78: 0.14477740392995586, 79: 0.15724421971890865, 80: 0.016601983093233984, 81: 0.07165487312140029, 82: 0.04507123561352697}\n"
     ]
    }
   ],
   "source": [
    "print(parameter.Loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
